<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="深度学习(线性回归模型)监督学习定义： ​	映射 x -&gt; y的算法（input -&gt;  output） 提供输入（source）和正确的输出（labels） 通过定义的算法 训练，让他变成仅接受输入就能预测或猜测出输出  回归和分类是深度学习中的两大主要任务方向。  回归：如前所述，回归任务主要用于预测连续的数值。它会输出一个实数，常见的应用包括房价预测、温度预测、销售额预测等。回归">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习(线性回归模型)">
<meta property="og:url" content="http://example.com/2024/10/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B)/index.html">
<meta property="og:site_name" content="AlexHome">
<meta property="og:description" content="深度学习(线性回归模型)监督学习定义： ​	映射 x -&gt; y的算法（input -&gt;  output） 提供输入（source）和正确的输出（labels） 通过定义的算法 训练，让他变成仅接受输入就能预测或猜测出输出  回归和分类是深度学习中的两大主要任务方向。  回归：如前所述，回归任务主要用于预测连续的数值。它会输出一个实数，常见的应用包括房价预测、温度预测、销售额预测等。回归">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2024-10-10T10:20:45.000Z">
<meta property="article:modified_time" content="2024-12-01T13:58:43.018Z">
<meta property="article:author" content="AlexQFMM">
<meta property="article:tag" content="人工智能">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/2024/10/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B)/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>深度学习(线性回归模型) | AlexHome</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">AlexHome</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-guestbook">

    <a href="/guestbook/" rel="section"><i class="fa fa-book fa-fw"></i>留言</a>

  </li>
        <li class="menu-item menu-item-mypage">

    <a href="/MYHTML/test/index.html" rel="section"><i class="fas fa-file-user fa-fw"></i>myPage</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/10/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B)/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/Avatar.jpg">
      <meta itemprop="name" content="AlexQFMM">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AlexHome">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          深度学习(线性回归模型)
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-10-10 10:20:45" itemprop="dateCreated datePublished" datetime="2024-10-10T10:20:45+00:00">2024-10-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-12-01 13:58:43" itemprop="dateModified" datetime="2024-12-01T13:58:43+00:00">2024-12-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E9%98%85%E8%AF%BB/" itemprop="url" rel="index"><span itemprop="name">阅读</span></a>
                </span>
            </span>

          
            <span id="/2024/10/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B)/" class="post-meta-item leancloud_visitors" data-flag-title="深度学习(线性回归模型)" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2024/10/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B)/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2024/10/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B)/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="深度学习-线性回归模型"><a href="#深度学习-线性回归模型" class="headerlink" title="深度学习(线性回归模型)"></a>深度学习(线性回归模型)</h1><h2 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h2><p><strong>定义：</strong></p>
<p>​	映射 x -&gt; y的算法（input -&gt;  output）</p>
<p>提供输入（source）和正确的输出（labels） 通过定义的算法 训练，让他变成仅接受输入就能预测或猜测出输出 </p>
<p><strong>回归和分类是深度学习中的两大主要任务方向。</strong></p>
<ol>
<li><strong>回归</strong>：如前所述，回归任务主要用于预测连续的数值。它会输出一个实数，常见的应用包括房价预测、温度预测、销售额预测等。回归分析帮助我们理解输入变量与输出变量之间的关系。</li>
<li><strong>分类</strong>：分类任务用于将输入数据分配到离散的类别中。常见的应用包括图像分类（例如，识别图片中的物体）、文本分类（例如，垃圾邮件检测）、情感分析等。在分类任务中，模型的输出通常是一个类别标签或者每个类别的概率。</li>
</ol>
<p>这两种任务在深度学习模型的设计、损失函数的选择及评估指标上有显著的区别。回归一般使用均方误差（MSE）作为损失函数，而分类则常用交叉熵损失。对于评估，回归常用均方根误差（RMSE）等指标，而分类则多用准确率、精确率、召回率等。</p>
<span id="more"></span>

<h2 id="非监督学习"><a href="#非监督学习" class="headerlink" title="非监督学习"></a>非监督学习</h2><p>非监督学习是一种在没有标签（labels）和明确目标输出的情况下进行学习的机器学习方法。其主要目标是识别和理解数据中的潜在模式、相似性或分组关系。以下是非监督学习的一些关键概念和应用领域：</p>
<ul>
<li><strong>聚类</strong>：聚类算法用于将相似的数据点分组。在市场营销中，可以通过聚类分析划分用户群体，以便准确制定营销策略。此外，在自然语言处理领域，可以将具有相似主题的文章进行分组。</li>
<li><strong>异常检测</strong>：非监督学习可用于识别数据中的异常行为或异常点。例如，在网络安全领域，算法可以检测到与正常模式不符的网络活动，从而识别潜在的安全威胁。在金融领域，可以发现异常交易活动，帮助预防欺诈。</li>
<li><strong>降维</strong>：降维技术用于将高维数据投影到低维空间，以便于可视化和进一步分析。主成分分析（PCA）和t-SNE是常用的降维技术，它们能够帮助研究者简化数据，同时保留大部分重要信息，使得数据更易于理解。</li>
</ul>
<p>非监督学习在处理未经标注的数据时具有广泛应用，可以发现数据中的新模式，并揭示潜在的结构，为后续的分析和决策提供支持。</p>
<h1 id="线性回归模型"><a href="#线性回归模型" class="headerlink" title="线性回归模型"></a>线性回归模型</h1><p><strong>linear regression</strong></p>
<h2 id="模型Model"><a href="#模型Model" class="headerlink" title="模型Model"></a>模型Model</h2><p>形如<br>$$<br>f_{w,b}（x） &#x3D; wx + b<br>$$<br> 的线性直线</p>
<h2 id="模型的参数-（parameners）"><a href="#模型的参数-（parameners）" class="headerlink" title="模型的参数 （parameners）"></a>模型的参数 （parameners）</h2><p>w,b </p>
<p>模型的参数指 可以在训练期间调整以改进模型的变量 ， 别名权重、系数</p>
<h2 id="成本函数"><a href="#成本函数" class="headerlink" title="成本函数"></a>成本函数</h2><p>别名 ： 代价函数</p>
<p>为了找到更贴近的w和b，我们将构造一个<strong>成本函数</strong></p>
<p><strong>平方误差成本函数</strong><br>$$<br>J(w,b) &#x3D; \frac{1}{2m} \sum_{i&#x3D;1}^{m} \left( f_{w,b}(x^{(i)}) - y^{(i)} \right)^2<br>$$</p>
<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><p>线性回归会尝试找到w，b的值，并尽可能让w，b最小即<br>$$<br>minmize J(w,b)<br>$$</p>
<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>梯度下降算法是从一个初始参数（如权重 <em>w</em> 和偏置 <em>b</em>）开始，通过迭代不断调整这些参数来最小化损失函数。然而，不同的初始参数可能导致梯度下降收敛到不同的局部最小值，尤其是在损失函数具有多个极小值的情况下。</p>
<h3 id="实现梯度下降"><a href="#实现梯度下降" class="headerlink" title="实现梯度下降"></a>实现梯度下降</h3><p>线性回归中公式如下，注意这里的等于号是 <strong>赋值</strong> 而非 相等<br>$$<br>w &#x3D; w - \alpha \frac{d}{dw} J(w, b)<br>$$</p>
<p>$$<br>b &#x3D; b - \alpha \frac{d}{db} J(w, b)<br>$$</p>
<p>求导后结果<br>$$<br>w &#x3D; \frac{1}{m} \sum_{i&#x3D;1}^{m} \left( f_{w,b}(x^{(i)}) - y^{(i)} \right)x^{(i)}<br>$$</p>
<p>$$<br>b &#x3D; \frac{1}{m} \sum_{i&#x3D;1}^{m} \left( f_{w,b}(x^{(i)}) - y^{(i)} \right)<br>$$</p>
<p>α：学习率（learning rate），通常是一个0~1之间的小正数</p>
<ul>
<li>alpha越大，梯度下降越激烈</li>
<li>太小走的慢，太大不一定能到最小值，卡在山谷的两边，甚至发散，离目标值越来越远</li>
</ul>
<p>​	</p>
<p>w,b两个应该<strong>同步更新</strong></p>
<p>导数向：表示了损失函数 在参数空间中的变化率和方向</p>
<ul>
<li>正值：如果某个参数的导数为正，说明增加该参数会导致损失增加。这意味着我们应该减少这个参数的值，以期降低损失。</li>
<li>负值：如果导数为负，说明增加该参数会导致损失减少。在这种情况下，我们应该增加这个参数的值。</li>
<li>零值：当导数为零时，表示此处可能是一个最优解（局部最低点或鞍点），因为在这一点上，损失函数不再变化。</li>
</ul>
<h2 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h2><p>有一些没见过的后续会提及，不用着急</p>
<p><strong>test.py</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np  </span><br><span class="line">import os  </span><br><span class="line">import torch  </span><br><span class="line">import torch.nn as nn  </span><br><span class="line">import torch.optim as optim  </span><br><span class="line">from torch.utils.tensorboard import SummaryWriter  </span><br><span class="line"></span><br><span class="line"># 读取数据  </span><br><span class="line">filedata = np.loadtxt(&#x27;test.txt&#x27;, dtype=float)  </span><br><span class="line">Areas = filedata[:, 0]  # 面积  </span><br><span class="line">Prices = filedata[:, 1]  # 房价  </span><br><span class="line"></span><br><span class="line"># 数据归一化  </span><br><span class="line">Areas = (Areas - np.mean(Areas)) / np.std(Areas)  </span><br><span class="line">Prices = (Prices - np.mean(Prices)) / np.std(Prices)  </span><br><span class="line"></span><br><span class="line"># 定义线性回归模型  </span><br><span class="line">class LinearModel(nn.Module):  </span><br><span class="line">    def __init__(self):  </span><br><span class="line">        super(LinearModel, self).__init__()  </span><br><span class="line">        self.linear = nn.Linear(1, 1)  # 输入一个特征，输出一个目标值  </span><br><span class="line"></span><br><span class="line">    def forward(self, x):  </span><br><span class="line">        return self.linear(x)  </span><br><span class="line"></span><br><span class="line"># 创建模型实例  </span><br><span class="line">model = LinearModel()  </span><br><span class="line"></span><br><span class="line"># 准备输入数据  </span><br><span class="line">areas_tensor = torch.from_numpy(Areas).float().view(-1, 1)  # 转换为 PyTorch 张量  </span><br><span class="line">prices_tensor = torch.from_numpy(Prices).float().view(-1, 1)  # 转换为 PyTorch 张量  </span><br><span class="line"></span><br><span class="line"># 定义损失函数和优化器  </span><br><span class="line">criterion = nn.MSELoss()  # 均方误差损失  </span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=0.001)  # 随机梯度下降优化器  </span><br><span class="line"></span><br><span class="line"># 创建 TensorBoard SummaryWriter  </span><br><span class="line">writer = SummaryWriter(&#x27;runs/linear_regression_experiment&#x27;)  </span><br><span class="line"></span><br><span class="line"># 记录模型图  </span><br><span class="line">dummy_input = torch.tensor([[0.0]], dtype=torch.float32)  # 模型输入的占位符  </span><br><span class="line">writer.add_graph(model, dummy_input)  </span><br><span class="line"></span><br><span class="line"># 记录实验描述  </span><br><span class="line">writer.add_text(&#x27;Experiment Description&#x27;, &#x27;This is a simple linear regression model training on normalized areas and prices.&#x27;)  </span><br><span class="line"></span><br><span class="line"># 训练模型  </span><br><span class="line">epochs = 500  </span><br><span class="line">for epoch in range(epochs):  </span><br><span class="line">    model.train()  # 将模型设置为训练模式  </span><br><span class="line"></span><br><span class="line">    # 前向传播  </span><br><span class="line">    optimizer.zero_grad()  # 清零梯度  </span><br><span class="line">    outputs = model(areas_tensor)  # 输出结果  </span><br><span class="line">    loss = criterion(outputs, prices_tensor)  # 计算损失  </span><br><span class="line"></span><br><span class="line">    # 反向传播和优化  </span><br><span class="line">    loss.backward()  # 反向传播  </span><br><span class="line">    optimizer.step()  # 更新参数  </span><br><span class="line"></span><br><span class="line">    # 在 TensorBoard 中记录损失  </span><br><span class="line">    writer.add_scalar(&#x27;Loss/train&#x27;, loss.item(), epoch)  </span><br><span class="line"></span><br><span class="line">    # 记录模型参数的直方图  </span><br><span class="line">    for name, param in model.named_parameters():  </span><br><span class="line">        writer.add_histogram(name, param.data, epoch)  </span><br><span class="line">        # 记录参数分布  </span><br><span class="line">        writer.add_histogram(f&#x27;&#123;name&#125;/distribution&#x27;, param.data, epoch)  </span><br><span class="line"></span><br><span class="line">    # 每100个epoch打印一次损失  </span><br><span class="line">    if epoch % 100 == 0:  </span><br><span class="line">        print(f&quot;Epoch &#123;epoch&#125;, Loss: &#123;loss.item()&#125;&quot;)  </span><br><span class="line"></span><br><span class="line"># 保存模型为.pt  </span><br><span class="line">torch.save(model.state_dict(), &#x27;linear_regression_model.pt&#x27;)  </span><br><span class="line"></span><br><span class="line"># 转换为 ONNX 格式  </span><br><span class="line">torch.onnx.export(model, dummy_input, &quot;linear_regression_model.onnx&quot;, verbose=True)  </span><br><span class="line"></span><br><span class="line">print(&quot;模型已保存为 &#x27;linear_regression_model.pt&#x27; 和 &#x27;linear_regression_model.onnx&#x27;&quot;)  </span><br><span class="line"></span><br><span class="line"># 关闭 TensorBoard writer  </span><br><span class="line">writer.close()  </span><br><span class="line"></span><br><span class="line"># 启动 TensorBoard，指定端口  </span><br><span class="line">os.system(&#x27;python -m tensorboard.main --logdir=&quot;runs/linear_regression_experiment&quot; --port=8888&#x27;)</span><br></pre></td></tr></table></figure>



<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">可以在浏览器中输入 http://localhost:8888/查看报表</span><br></pre></td></tr></table></figure>



<p><strong>创建pytroch模型</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np  </span><br><span class="line">import torch  </span><br><span class="line">import torch.nn as nn  </span><br><span class="line">import torch.optim as optim  </span><br><span class="line"></span><br><span class="line"># 读取数据  </span><br><span class="line">filedata = np.loadtxt(&#x27;test.txt&#x27;, dtype=float)  </span><br><span class="line">Areas = filedata[:, 0]  # 面积  </span><br><span class="line">Prices = filedata[:, 1]  # 房价  </span><br><span class="line"></span><br><span class="line"># 数据归一化  </span><br><span class="line">Areas = (Areas - np.mean(Areas)) / np.std(Areas)  </span><br><span class="line">Prices = (Prices - np.mean(Prices)) / np.std(Prices)  </span><br><span class="line"></span><br><span class="line"># 定义线性回归模型  </span><br><span class="line">class LinearModel(nn.Module):  </span><br><span class="line">    def __init__(self):  </span><br><span class="line">        super(LinearModel, self).__init__()  </span><br><span class="line">        self.linear = nn.Linear(1, 1)  # 输入一个特征，输出一个目标值  </span><br><span class="line"></span><br><span class="line">    def forward(self, x):  </span><br><span class="line">        return self.linear(x)  </span><br><span class="line"></span><br><span class="line"># 创建模型实例  </span><br><span class="line">model = LinearModel()  </span><br><span class="line"></span><br><span class="line"># 准备输入数据  </span><br><span class="line">areas_tensor = torch.from_numpy(Areas).float().view(-1, 1)  # 转换为 PyTorch 张量  </span><br><span class="line">prices_tensor = torch.from_numpy(Prices).float().view(-1, 1)  # 转换为 PyTorch 张量  </span><br><span class="line"></span><br><span class="line"># 定义损失函数和优化器  </span><br><span class="line">criterion = nn.MSELoss()  # 均方误差损失  </span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=0.001)  # 随机梯度下降优化器  </span><br><span class="line"></span><br><span class="line"># 训练模型  </span><br><span class="line">epochs = 500  </span><br><span class="line">for epoch in range(epochs):  </span><br><span class="line">    model.train()  # 将模型设置为训练模式  </span><br><span class="line"></span><br><span class="line">    # 前向传播  </span><br><span class="line">    optimizer.zero_grad()  # 清零梯度  </span><br><span class="line">    outputs = model(areas_tensor)  # 输出结果  </span><br><span class="line">    loss = criterion(outputs, prices_tensor)  # 计算损失  </span><br><span class="line"></span><br><span class="line">    # 反向传播和优化  </span><br><span class="line">    loss.backward()  # 反向传播  </span><br><span class="line">    optimizer.step()  # 更新参数  </span><br><span class="line"></span><br><span class="line">    # 每100个epoch打印一次损失  </span><br><span class="line">    if epoch % 100 == 0:  </span><br><span class="line">        print(f&quot;Epoch &#123;epoch&#125;, Loss: &#123;loss.item()&#125;&quot;)  </span><br><span class="line"></span><br><span class="line"># 保存模型为.pt  </span><br><span class="line">torch.save(model.state_dict(), &#x27;linear_regression_model.pt&#x27;)  </span><br><span class="line"></span><br><span class="line"># 转换为 ONNX 格式  </span><br><span class="line">dummy_input = torch.tensor([[0.0]], dtype=torch.float32)  # 模型输入的占位符  </span><br><span class="line">torch.onnx.export(model, dummy_input, &quot;linear_regression_model.onnx&quot;, verbose=True)  </span><br><span class="line"></span><br><span class="line">print(&quot;模型已保存为 &#x27;linear_regression_model.pt&#x27; 和 &#x27;linear_regression_model.onnx&#x27;&quot;)</span><br></pre></td></tr></table></figure>



<p><strong>测试pt模型正确性</strong></p>
<p>testpt.py</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">import sys  </span><br><span class="line">import numpy as np  </span><br><span class="line">import torch  </span><br><span class="line">import torch.nn as nn  </span><br><span class="line"></span><br><span class="line"># 定义线性回归模型  </span><br><span class="line">class LinearModel(nn.Module):  </span><br><span class="line">    def __init__(self):  </span><br><span class="line">        super(LinearModel, self).__init__()  </span><br><span class="line">        self.linear = nn.Linear(1, 1)  # 输入一个特征，输出一个目标值  </span><br><span class="line"></span><br><span class="line">    def forward(self, x):  </span><br><span class="line">        return self.linear(x)  </span><br><span class="line"></span><br><span class="line">def load_model(model_path):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    加载模型  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    model = LinearModel()  </span><br><span class="line">    model.load_state_dict(torch.load(model_path))  </span><br><span class="line">    model.eval()  # 设置为评估模式  </span><br><span class="line">    return model  </span><br><span class="line"></span><br><span class="line">def predict_price(model, area, filedata):  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    预测房价  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    # 归一化输入  </span><br><span class="line">    areas = filedata[:, 0]  </span><br><span class="line">    area_normalized = (area - np.mean(areas)) / np.std(areas)  </span><br><span class="line"></span><br><span class="line">    # 将输入转换为 PyTorch 的张量  </span><br><span class="line">    area_tensor = torch.tensor([[area_normalized]], dtype=torch.float32)  </span><br><span class="line"></span><br><span class="line">    # 进行预测  </span><br><span class="line">    with torch.no_grad():  </span><br><span class="line">        predicted_price_normalized = model(area_tensor)  </span><br><span class="line"></span><br><span class="line">    # 反归一化输出  </span><br><span class="line">    prices = filedata[:, 1]  </span><br><span class="line">    predicted_price = predicted_price_normalized.item() * np.std(prices) + np.mean(prices)  </span><br><span class="line">    </span><br><span class="line">    return predicted_price  </span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:  </span><br><span class="line">    if len(sys.argv) != 3:  </span><br><span class="line">        print(&quot;用法: python predict.py &lt;model_path&gt; &lt;area&gt;&quot;)  </span><br><span class="line">        sys.exit(1)  </span><br><span class="line"></span><br><span class="line">    model_path = sys.argv[1]  </span><br><span class="line">    area = float(sys.argv[2])  </span><br><span class="line"></span><br><span class="line">    # 读取数据  </span><br><span class="line">    filedata = np.loadtxt(&#x27;test.txt&#x27;)  </span><br><span class="line"></span><br><span class="line">    # 加载模型  </span><br><span class="line">    model = load_model(model_path)  </span><br><span class="line"></span><br><span class="line">    # 进行预测  </span><br><span class="line">    price = predict_price(model, area, filedata)  </span><br><span class="line"></span><br><span class="line">    # 打印结果  </span><br><span class="line">    print(f&quot;预测房价(面积 &#123;area&#125; 平方米): &#123;price&#125; 万&quot;)</span><br></pre></td></tr></table></figure>



<p><strong>测试onnx模型正确性</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;  </span><br><span class="line">#include &lt;vector&gt;  </span><br><span class="line">#include &lt;string&gt;  </span><br><span class="line">#include &lt;opencv2/opencv.hpp&gt;  </span><br><span class="line">#include &lt;opencv2/dnn/dnn.hpp&gt;  </span><br><span class="line"></span><br><span class="line">using cv::Mat;  </span><br><span class="line">using std::vector;  </span><br><span class="line">using std::string;  </span><br><span class="line">using std::cout;  </span><br><span class="line">using std::endl;  </span><br><span class="line"></span><br><span class="line">// 从您计算的结果中填入均值和标准差  </span><br><span class="line">const float mean_area = 113.7988f; // 房屋面积均值  </span><br><span class="line">const float std_area = 45.4836f;   // 房屋面积标准差  </span><br><span class="line">const float mean_price = 1176.76f; // 房价均值  </span><br><span class="line">const float std_price = 1118.3278f; // 房价标准差 </span><br><span class="line"></span><br><span class="line">int main(int argc, char* argv[]) &#123;  </span><br><span class="line">    if (argc != 2) &#123;  </span><br><span class="line">        cout &lt;&lt; &quot;Usage: &quot; &lt;&lt; argv[0] &lt;&lt; &quot; &lt;area&gt;&quot; &lt;&lt; endl;  </span><br><span class="line">        return -1;  </span><br><span class="line">    &#125;  </span><br><span class="line"></span><br><span class="line">    // 解析输入参数  </span><br><span class="line">    float area = std::stof(argv[1]);  </span><br><span class="line"></span><br><span class="line">    // 读取 ONNX 模型  </span><br><span class="line">    cv::dnn::Net net = cv::dnn::readNetFromONNX(&quot;linear_regression_model.onnx&quot;);   </span><br><span class="line"></span><br><span class="line">    // 归一化输入数据  </span><br><span class="line">    float normalized_area = (area - mean_area) / std_area;  </span><br><span class="line"></span><br><span class="line">    // 创建输入张量  </span><br><span class="line">    Mat inputBlob = Mat(1, 1, CV_32F, normalized_area);  </span><br><span class="line">    net.setInput(inputBlob);  </span><br><span class="line"></span><br><span class="line">    // 执行推理  </span><br><span class="line">    Mat output = net.forward();  </span><br><span class="line"></span><br><span class="line">    // 反归一化输出  </span><br><span class="line">    float predicted_price_normalized = output.at&lt;float&gt;(0, 0);  </span><br><span class="line">    float predicted_price = predicted_price_normalized * std_price + mean_price;  </span><br><span class="line"></span><br><span class="line">    // 输出结果  </span><br><span class="line">    cout &lt;&lt; &quot;预测房价(面积 &quot; &lt;&lt; area &lt;&lt; &quot; 平方米): &quot; &lt;&lt; predicted_price &lt;&lt; &quot; 万&quot; &lt;&lt; endl;  </span><br><span class="line"></span><br><span class="line">    return 0;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>追加：</p>
<p><strong>计算均差和标准差</strong></p>
<p>tmp.py</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np  </span><br><span class="line"></span><br><span class="line"># 读取数据  </span><br><span class="line">data = np.loadtxt(&#x27;test.txt&#x27;)  </span><br><span class="line"></span><br><span class="line"># 计算均值和标准差  </span><br><span class="line">mean_area = np.mean(data[:, 0])  # 房屋面积的均值  </span><br><span class="line">std_area = np.std(data[:, 0])    # 房屋面积的标准差  </span><br><span class="line">mean_price = np.mean(data[:, 1]) # 房价的均值  </span><br><span class="line">std_price = np.std(data[:, 1])   # 房价的标准差  </span><br><span class="line"></span><br><span class="line">print(&quot;Mean Area:&quot;, mean_area)  </span><br><span class="line">print(&quot;Std Area:&quot;, std_area)  </span><br><span class="line">print(&quot;Mean Price:&quot;, mean_price)  </span><br><span class="line">print(&quot;Std Price:&quot;, std_price)</span><br></pre></td></tr></table></figure>



<h1 id="多输入特征的多元线性回归"><a href="#多输入特征的多元线性回归" class="headerlink" title="多输入特征的多元线性回归"></a>多输入特征的多元线性回归</h1><p>之前说的线性回归模型只有单一特征值，现在他有多特征，用<strong>下标</strong>表示即<br>$$<br>x_i<br>$$<br>ps:分清楚上标和下标的区别，上标指是集合中第i个元素，下标指元素的第i个特征</p>
<p>比如第四个数据的第2个特征，我们用下面形式表示<br>$$<br>x_4^{(2)}<br>$$</p>
<h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>$$<br>f_{w,b}(x) &#x3D; w \cdot x +b<br>$$</p>
<p>ps：这里是<strong>点积</strong></p>
<h2 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h2><p>python调用numpy库</p>
<p>实现向量点积</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f = np.dot(w,x) + b</span><br></pre></td></tr></table></figure>

<p>向量化的速度比直接用for循环要快的多</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for i in range(0,n)</span><br><span class="line">	f += w[i] * x[i]</span><br><span class="line">f += b</span><br></pre></td></tr></table></figure>

<p>why?</p>
<ol>
<li><strong>减少了Python解释器的开销</strong>：在循环中，每次迭代都需要解释器进行上下文切换和多次函数调用，而在向量化操作中，这些开销大大减少，因为整个操作是在底层C或Fortran实现的，调用效率更高。</li>
<li><strong>使用优化的底层实现</strong>：NumPy等科学计算库通常使用底层的优化算法（例如，BLAS和LAPACK）来处理向量和矩阵运算。这些算法经过高度优化，可以利用现代CPU的特性，例如SIMD（单指令多数据），从而加速运算。</li>
<li><strong>内存访问模式</strong>：向量化操作通常会更好地利用缓存，因为它可以在连续的内存块中处理数据。相比之下，循环可能导致更多的随机内存访问，这会导致缓存不命中，从而降低性能。</li>
<li><strong>最主要一点</strong>，np.dot实现了并行计算，并使用专门的硬件进行相加</li>
</ol>
<h2 id="梯度下降-1"><a href="#梯度下降-1" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>$$<br>w_j &#x3D; w_j - \alpha\frac{\partial}{\partial w_j} J(w_1, \ldots, w_n, b)<br>$$</p>
<p>$$<br>w_j &#x3D; w_j - \alpha\frac{\partial}{\partial b} J(w_1, \ldots, w_n, b)<br>$$</p>
<p>ps:这个长得像6的是偏导数</p>
<h2 id="特征缩放（归一化）"><a href="#特征缩放（归一化）" class="headerlink" title="特征缩放（归一化）"></a>特征缩放（归一化）</h2><p>不同特征可能具有不同的取值范围，当特征值的差异很大时，梯度下降等优化算法可能会在某一方向上反应过度，导致反复 oscillation（震荡）而无法快速收敛，这会延长训练时间这会导致在训练模型时。</p>
<p>解决办法 ：</p>
<p>​	 <strong>归一化</strong></p>
<p>​	通过特征缩放，可以将数据统一到一个相似的尺度，比如将特征值归一化到0到1之间</p>
<p>​	<strong>标准化</strong>（Z-score Normalization）</p>
<p>​	每个数减去平均数，再除以标准差（方差的平方根）<br>$$<br>x_i &#x3D; \frac {(x_i - \mu _i)} {\sigma _i}<br>$$<br>​	我们常用以下字符表示标准差（sigma）<br>$$<br>\sigma<br>$$<br>​	以下字符表示平均值（mu）<br>$$<br>\mu<br>$$<br>​	python代码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = (x - np.mean(x))/np.std(x)</span><br><span class="line"></span><br><span class="line">x是向量数组</span><br></pre></td></tr></table></figure>



<h2 id="检测梯度下降是否收敛"><a href="#检测梯度下降是否收敛" class="headerlink" title="检测梯度下降是否收敛"></a>检测梯度下降是否收敛</h2><p>1、看学习曲线（learning curve）</p>
<p>x轴为训练次数</p>
<p>y轴为J(w,b)</p>
<p>若是下降的曲线则是收敛的</p>
<h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><p>特征工程是机器学习和数据挖掘过程中一个关键的步骤，旨在通过选择、修改和创建特征来提高模型的性能。特征是模型输入的数据属性，特征工程的质量直接影响模型的效果。</p>
<h3 id="1-特征选择（Feature-Selection）"><a href="#1-特征选择（Feature-Selection）" class="headerlink" title="1. 特征选择（Feature Selection）"></a>1. 特征选择（Feature Selection）</h3><ul>
<li><strong>目的</strong>：从现有特征中挑选出对模型预测最有帮助的特征，减少冗余和噪声，提高模型的泛化能力。</li>
<li>方法<ul>
<li>滤波法（Filter method）：使用统计测试和相关性分析来评估特征。</li>
<li>包装法（Wrapper method）：使用某种机器学习算法评估特征子集。</li>
<li>嵌入法（Embedded method）：结合模型学习过程进行特征选择，例如Lasso回归。</li>
</ul>
</li>
</ul>
<h3 id="2-特征提取（Feature-Extraction）"><a href="#2-特征提取（Feature-Extraction）" class="headerlink" title="2. 特征提取（Feature Extraction）"></a>2. 特征提取（Feature Extraction）</h3><ul>
<li><strong>目的</strong>：通过原始数据生成新的特征，通常用于减少维度，保持必要的信息。</li>
<li>方法<ul>
<li>主成分分析（PCA）：将高维数据映射到低维空间。</li>
<li>线性判别分析（LDA）：试图找到最佳投影方向以提高类间分离。</li>
</ul>
</li>
</ul>
<h3 id="3-特征构造（Feature-Engineering）"><a href="#3-特征构造（Feature-Engineering）" class="headerlink" title="3. 特征构造（Feature Engineering）"></a>3. 特征构造（Feature Engineering）</h3><ul>
<li><strong>目的</strong>：基于现有特征生成新的特征，以增强模型的表达能力。</li>
<li>方法<ul>
<li>数学变换：如对数变换、平方、平方根等。</li>
<li>类别编码：将类别变量转换为数值形式（如独热编码、标签编码）。</li>
<li>交互特征：构造特征之间的交互项（如乘积、比率等）。</li>
</ul>
</li>
</ul>
<h3 id="4-数据清洗（Data-Cleaning）"><a href="#4-数据清洗（Data-Cleaning）" class="headerlink" title="4. 数据清洗（Data Cleaning）"></a>4. 数据清洗（Data Cleaning）</h3><ul>
<li><strong>目的</strong>：处理缺失值、异常值和噪声，以提高数据质量。</li>
<li>方法<ul>
<li>处理缺失值：填补缺失值、删除含缺失值的样本等。</li>
<li>异常检测和处理：识别并处理不合逻辑或极端的数据点。</li>
</ul>
</li>
</ul>
<h3 id="5-特征缩放（Feature-Scaling）"><a href="#5-特征缩放（Feature-Scaling）" class="headerlink" title="5. 特征缩放（Feature Scaling）"></a>5. 特征缩放（Feature Scaling）</h3><ul>
<li><strong>目的</strong>：将特征调整到相似的尺度，以避免某些特征在模型训练中占主导地位。</li>
<li>方法<ul>
<li>归一化（Normalization）：将数据调整到 0 到 1 之间。</li>
<li>标准化（Standardization）：将数据调整为均值为0、标准差为1的分布。</li>
</ul>
</li>
</ul>
<h1 id="多项式回归概述"><a href="#多项式回归概述" class="headerlink" title="多项式回归概述"></a>多项式回归概述</h1><p><strong>多项式回归（Polynomial Regression）</strong>是一种扩展传统线性回归的方法，用于建立输入特征与输出目标之间的非线性关系。它通过引入多项式特征，可以有效地拟合复杂的数据模式。</p>
<h2 id="1-特征工程在多项式回归中的作用"><a href="#1-特征工程在多项式回归中的作用" class="headerlink" title="1. 特征工程在多项式回归中的作用"></a>1. 特征工程在多项式回归中的作用</h2><p>特征工程对于多项式回归的成功至关重要，主要包括以下方面：</p>
<ul>
<li><strong>特征构造</strong>：<ul>
<li>通过将原始特征 x 转换为其高次项（例如 x^2,x^3等），增强模型的表达能力。</li>
</ul>
</li>
<li><strong>特征选择</strong>：<ul>
<li>精心选择高次项的级别，以避免过拟合。过多的高次特征可能导致模型的复杂性增高，从而降低模型在新数据上的表现。可以使用交叉验证等方法来评估不同多项式次数的效果。</li>
</ul>
</li>
<li><strong>交互特征</strong>：<ul>
<li>在某些情况下，可以考虑特征间的交互作用，这有助于捕捉变量之间的关联性。</li>
</ul>
</li>
<li><strong>数据理解</strong>：<ul>
<li>对数据的特点和背景知识的理解，可以帮助决定哪些特征需要上升到高次，以更好地拟合数据。</li>
</ul>
</li>
</ul>
<h2 id="2-应用场景"><a href="#2-应用场景" class="headerlink" title="2. 应用场景"></a>2. 应用场景</h2><p>多项式回归常用于以下情境：</p>
<ul>
<li>数据呈现出明显的非线性关系。</li>
<li>需要在模型中考虑高阶效应。</li>
<li>希望在保持可解释性的同时捕获更多的复杂性。</li>
</ul>
<h3 id="特征的次方"><a href="#特征的次方" class="headerlink" title="特征的次方"></a>特征的次方</h3><ul>
<li><strong>非线性关系</strong>：特征的次方（如 x1^2）可以帮助模型捕捉到特征与输出之间的非线性关系。例如，房子的面积平方可能更好地反映价格的变化趋势。</li>
<li><strong>曲线拟合</strong>：通过增加次方项，模型可以拟合更复杂的曲线，而不仅仅是直线。</li>
</ul>
<h3 id="特征的相乘（交互项）"><a href="#特征的相乘（交互项）" class="headerlink" title="特征的相乘（交互项）"></a>特征的相乘（交互项）</h3><ul>
<li><strong>特征交互</strong>：交互项（如 x<em>1⋅</em>x2）用于表示两个特征之间的相互作用。例如，面积和地理位置的交互可能影响房价的合理性。</li>
<li><strong>复杂关系</strong>：这些项可以揭示单个特征无法单独捕捉到的复杂关系。</li>
</ul>
<h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><p>预测售价 售价 &#x3D; 面积 + 地理位置 + 房间数量 </p>
<p>很明显，面积和地理位置是相辅相成的， 但房间数量是不能引发线性关系的 那我就可以设成如下方程<br>$$<br>y(x) &#x3D; wx_1x_2 + x_3^k<br>$$<br>其中</p>
<p>​	</p>
<ul>
<li>**偶数次方（k&#x3D;2,4,…**：<ul>
<li>表示特征增加时售价影响为正向，且以平滑且加速的方式提升，适合描述稳定的增长。</li>
</ul>
</li>
<li>**奇数次方（k&#x3D;1,3,…**：<ul>
<li>可能表现出复杂的正向和负向影响，适合用来捕捉某些特征与结果之间的非线性关系，并且可能存在一定的上下波动。</li>
</ul>
</li>
</ul>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>多项式回归是一种强大的回归分析方法，通过引入多项式高次项，可以有效处理非线性关系。特征工程在多项式回归中扮演着重要的角色，适当的特征构造和选择能够显著改善模型的表现和准确性。然而，必须谨慎选择多项式的复杂度，以防止过拟合并确保模型的可解释性。</p>
<p>虽然多项式回归在某些特定领域（如物理科学、经济学等）仍然有其应用价值，但在实际的机器学习和数据分析场景中，它<strong>并不是最常用</strong>的模型。在选择模型时，通常会优先考虑更灵活、更易于处理的非线性模型。因此，在处理复杂数据时，考虑其他更先进和更棒的模型通常是一个更为常见的选择。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" rel="tag"># 人工智能</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/10/08/shell%E8%84%9A%E6%9C%AC/" rel="prev" title="linux查漏补缺">
      <i class="fa fa-chevron-left"></i> linux查漏补缺
    </a></div>
      <div class="post-nav-item">
    <a href="/2024/10/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0(l%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B)/" rel="next" title="深度学习(逻辑回归模型)">
      深度学习(逻辑回归模型) <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.</span> <span class="nav-text">深度学习(线性回归模型)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.1.</span> <span class="nav-text">监督学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.2.</span> <span class="nav-text">非监督学习</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.</span> <span class="nav-text">线性回归模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8BModel"><span class="nav-number">2.1.</span> <span class="nav-text">模型Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8F%82%E6%95%B0-%EF%BC%88parameners%EF%BC%89"><span class="nav-number">2.2.</span> <span class="nav-text">模型的参数 （parameners）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%88%90%E6%9C%AC%E5%87%BD%E6%95%B0"><span class="nav-number">2.3.</span> <span class="nav-text">成本函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9B%AE%E6%A0%87"><span class="nav-number">2.4.</span> <span class="nav-text">目标</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">2.5.</span> <span class="nav-text">梯度下降</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">2.5.1.</span> <span class="nav-text">实现梯度下降</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.6.</span> <span class="nav-text">构建模型</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%A4%9A%E8%BE%93%E5%85%A5%E7%89%B9%E5%BE%81%E7%9A%84%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-number">3.</span> <span class="nav-text">多输入特征的多元线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Model"><span class="nav-number">3.1.</span> <span class="nav-text">Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%90%91%E9%87%8F%E5%8C%96"><span class="nav-number">3.2.</span> <span class="nav-text">向量化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D-1"><span class="nav-number">3.3.</span> <span class="nav-text">梯度下降</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE%EF%BC%88%E5%BD%92%E4%B8%80%E5%8C%96%EF%BC%89"><span class="nav-number">3.4.</span> <span class="nav-text">特征缩放（归一化）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A3%80%E6%B5%8B%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%98%AF%E5%90%A6%E6%94%B6%E6%95%9B"><span class="nav-number">3.5.</span> <span class="nav-text">检测梯度下降是否收敛</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B"><span class="nav-number">3.6.</span> <span class="nav-text">特征工程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%EF%BC%88Feature-Selection%EF%BC%89"><span class="nav-number">3.6.1.</span> <span class="nav-text">1. 特征选择（Feature Selection）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%EF%BC%88Feature-Extraction%EF%BC%89"><span class="nav-number">3.6.2.</span> <span class="nav-text">2. 特征提取（Feature Extraction）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E7%89%B9%E5%BE%81%E6%9E%84%E9%80%A0%EF%BC%88Feature-Engineering%EF%BC%89"><span class="nav-number">3.6.3.</span> <span class="nav-text">3. 特征构造（Feature Engineering）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%EF%BC%88Data-Cleaning%EF%BC%89"><span class="nav-number">3.6.4.</span> <span class="nav-text">4. 数据清洗（Data Cleaning）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE%EF%BC%88Feature-Scaling%EF%BC%89"><span class="nav-number">3.6.5.</span> <span class="nav-text">5. 特征缩放（Feature Scaling）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92%E6%A6%82%E8%BF%B0"><span class="nav-number">4.</span> <span class="nav-text">多项式回归概述</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E5%9C%A8%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92%E4%B8%AD%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="nav-number">4.1.</span> <span class="nav-text">1. 特征工程在多项式回归中的作用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="nav-number">4.2.</span> <span class="nav-text">2. 应用场景</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E7%9A%84%E6%AC%A1%E6%96%B9"><span class="nav-number">4.2.1.</span> <span class="nav-text">特征的次方</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E7%9A%84%E7%9B%B8%E4%B9%98%EF%BC%88%E4%BA%A4%E4%BA%92%E9%A1%B9%EF%BC%89"><span class="nav-number">4.2.2.</span> <span class="nav-text">特征的相乘（交互项）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E4%BE%8B"><span class="nav-number">4.2.3.</span> <span class="nav-text">实例</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%93%E8%AE%BA"><span class="nav-number">4.3.</span> <span class="nav-text">结论</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="AlexQFMM"
      src="/images/Avatar.jpg">
  <p class="site-author-name" itemprop="name">AlexQFMM</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">28</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://qexo-ten-roan.vercel.app/" title="控制中心 → https:&#x2F;&#x2F;qexo-ten-roan.vercel.app&#x2F;" rel="noopener" target="_blank">控制中心</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">AlexQFMM</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : true,
      appId      : 'UaEUwcYLlkREnSn9jPNe0oRf-gzGzoHsz',
      appKey     : '5d7KwGdf31jiCBxBobH0wVk6',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : true,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginModelPath":"assets/","model":{"jsonPath":"/live2dw/assets/assets/miku.model.json"},"display":{"position":"left","width":200,"height":400},"mobile":{"show":true},"log":false,"pluginJsPath":"lib/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
