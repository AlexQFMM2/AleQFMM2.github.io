---
title: 深度学习(神经网络)
categories: 阅读
date: 2024-10-12 10:20:45
tags: 人工智能
---

# 深度学习(神经网络)

神经网络和传统机器学习的区别主要体现在以下几个方面：

1. **模型结构**：
   - **神经网络**：由多个层（输入层、隐藏层、输出层）组成，层与层之间由神经元连接，能够自动学习输入数据的特征表示。
   - **传统机器学习**：模型通常较为简单，如线性回归、决策树、支持向量机等，依赖于特征工程，手动提取特征。
2. **特征提取**：
   - **神经网络**：具备自动特征提取能力，能够从原始数据中自动学习到有用的特征，尤其在图像、语音等复杂数据上表现突出。
   - **传统机器学习**：通常需要专家手动进行特征选择和提取，过程中容易受到主观因素的影响。
3. **处理数据的能力**：
   - **神经网络**：适用于大规模数据，可以处理高维的数据，如图像和音频，适合深度学习任务。
   - **传统机器学习**：在小规模数据上通常表现良好，但在数据量极大时可能效果不佳。
4. **训练过程**：
   - **神经网络**：通过反向传播算法进行训练，通常需要更多的计算资源和时间，但能够在复杂任务中取得更好的性能。
   - **传统机器学习**：训练通常较快，可以使用较少的计算资源，适用于线性或结构简单的任务。
5. **泛化能力**：
   - **神经网络**：由于模型复杂度高，容易出现过拟合现象，因此需要正则化和更多的数据来提升泛化能力。
   - **传统机器学习**：在数据量较少时，通常能更好地泛化，但也容易受到特征选择的影响。

<!--more-->

# 神经网络中的层

神经网络的层是组成神经网络模型的基本单元，每一层由多个神经元（或节点）组成。这些层通常可以分为以下几种类型：

1. **输入层**（Input Layer）：
   - 这是神经网络的第一层，用于接收输入数据。每个输入特征对应一个神经元。输入层不进行任何计算，它只负责将数据传递给下一个层。
2. **隐藏层**（Hidden Layers）：
   - 输入层和输出层之间的层称为隐藏层。可以有多个隐藏层，每一层由多个神经元组成。隐藏层通过加权和激活函数对输入数据进行处理和转换，提取数据中的特征。隐藏层的数量和神经元的数量会影响网络的学习能力和表达能力。
3. **输出层**（Output Layer）：
   - 这是神经网络的最后一层，用于生成模型的输出结果。输出层的神经元数量通常与目标变量的数量相对应。在分类任务中，输出层可能使用softmax激活函数以生成概率分布；在回归任务中，可能使用线性激活函数以输出连续值。

### 激活函数

每一层的神经元通常会经过激活函数进行非线性转换。常见的激活函数包括：

- **Sigmoid**：将输出限制在（0, 1）之间，常用于二分类任务。
- **ReLU（Rectified Linear Unit）**：在输入大于0时输出自身，否则输出0，通常用于隐藏层。
- **Tanh**：将输出限制在（-1, 1）之间，常用于结果需要 标准化的任务。

### 层的类型

除了基本的输入、隐藏和输出层，还有其他特殊类型的层：

- **卷积层（Convolutional Layer）**：用于处理图像数据，具有局部连接和权重共享特性。
- **池化层（Pooling Layer）**：用于降低特征的维度和计算复杂度，常用于卷积神经网络中。
- **递归层（Recurrent Layer）**：用于处理序列数据，具有记忆特性，如LSTM（长短期记忆）和GRU（门控递归单元）。
- **全连接层（Fully Connected Layer）**：每个神经元与前一层的所有神经元相连，通常用于连接输入和输出。

我们用以下形式表达神经网路层号
$$
w^{[i]}
$$
这代表第i层



### 推理：向前传播

推理或向前传播是神经网络从输入到输出的过程。在这个过程中，输入数据通过网络的每一层，逐层进行计算，直到产生最终输出。

在Python中实现前向传播时，需定义一个稠密函数

#### 稠密函数

是神经网络中的基本组成部分之一，用于将输入数据的所有特征与输出层的每个神经元相连接

神经网络的前向传播过程中，通常会先通过稠密函数（即全连接层）对输入进行线性变换，然后再通过激活函数将输出转换为非线性形式。

- **权重与偏置**：可以提及权重和偏置在稠密函数中的重要性，权重决定了特征的重要性，偏置则有助于更好地调整模型的输出。



### 训练：反向传播

反向传播是训练神经网络的关键步骤，通过计算损失函数相对于权重的梯度，根据计算的梯度来更新权重。



反向传播是训练神经网络的关键步骤，通过计算损失函数相对于权重的梯度，根据计算的梯度来更新权重。



# 整体流程框架

以下是使用 **PyTorch** 框架进行推理流程、数据处理和构建神经网络的基本步骤。

### 1. 数据处理

在使用 PyTorch 进行深度学习时，数据准备是重要的第一步。通常包括以下几个步骤：

- **导入必要的库**：

```
import torch  
from torchvision import datasets, transforms  
from torch.utils.data import DataLoader  
```

- **数据预处理**：

通过 `torchvision` 的 `transforms` 工具进行数据转换（如归一化、数据增强等）。

```
transform = transforms.Compose([  
    transforms.Resize((28, 28)),  
    transforms.ToTensor(),  
    transforms.Normalize((0.5,), (0.5,))  # 对于单通道灰度图像  
])  
```

- **加载数据集**：

使用 `datasets` 获取数据，并使用 `DataLoader` 创建数据加载器。

```
train_dataset = datasets.MNIST(root='data', train=True, download=True, transform=transform)  
train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)  

test_dataset = datasets.MNIST(root='data', train=False, download=True, transform=transform)  
test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)  
```

### 2.数据的格式

在 PyTorch 中，数据通常以张量（tensor）的形式表示。

- **0维张量**（标量）：一个独立的珠子（单个值）。
- **1维张量**（向量）：一条线上的珠子（一个一维数组）。
- **2维张量**（矩阵）：一张表格，行和列都放了珠子（一个二维数组）。
- **3维张量**：一个立方体的珠子（一个包含多个表格的三维结构）。

### 3. 构建神经网络

使用 PyTorch 的 `nn` 模块来构建神经网络。以下是一个简单的完全连接的神经网络示例：

```
import torch.nn as nn  
import torch.nn.functional as F  

class SimpleNN(nn.Module):  
    def __init__(self):  
        super(SimpleNN, self).__init__()  
        self.fc1 = nn.Linear(28 * 28, 128)  # 输入层到隐藏层  
        self.fc2 = nn.Linear(128, 64)        # 隐藏层到隐藏层  
        self.fc3 = nn.Linear(64, 10)         # 隐藏层到输出层  

    def forward(self, x):  
        x = x.view(-1, 28 * 28)  # 展平输入  
        x = F.relu(self.fc1(x))   # 第一层激活  
        x = F.relu(self.fc2(x))   # 第二层激活  
        x = self.fc3(x)           # 输出层  
        return x  

model = SimpleNN()  
```

### 3. 推理流程（向前传播）

在模型训练之后，可以通过推理得到模型的 输出。以下是推理的步骤：

- **模型设为评估模式**：

```
model.eval()  # 设定模型为评估模式  
```

- **进行推理**：

```
with torch.no_grad():  # 不需要计算梯度  
    for images, labels in test_loader:  
        outputs = model(images)  # 向前传播  
        # 处理输出，例如计算预测类、准确度等  
```

### 4. 反向传播

在训练中，模型需要通过反向传播来更新权重。以下是训练的基本步骤：

- **损失函数和优化器设置**：

```
criterion = nn.CrossEntropyLoss()  # 损失函数  
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # 优化器  
```

- **训练循环**：

```
num_epochs = 5  # 设定训练的轮数  

for epoch in range(num_epochs):  
    model.train()  # 设定模型为训练模式  
    for images, labels in train_loader:  
        optimizer.zero_grad()  # 清零梯度  

        outputs = model(images)  # 向前传播  
        loss = criterion(outputs, labels)  # 计算损失  
        loss.backward()  # 反向传播  
        optimizer.step()  # 更新权重  
```

以上是使用 PyTorch 进行推理流程、数据处理和构建神经网络的基本步骤。

## 扩展：

**Ai可以分为ANI和AGI两部分**

### ANI（窄域人工智能）

1. **定义**：
   - ANI是指专门为特定任务或应用设计的人工智能系统。它们在某一特定领域具有高度的专业能力，但无法超出该领域执行其他任务。
2. **特征**：
   - **任务专注**：ANI系统能够在特定的任务中表现得比人类更优秀，例如图像识别、语音识别、自然语言处理等。
   - **有限性**：虽然能力强，但对新环境或新任务的适应性差。ANI无法从一个任务泛化到另一任务。
   - **实例**：常见的 ANI 应用包括聊天机器人、推荐系统（如Netflix和Spotify的推荐引擎）、语音助手（如Siri和Alexa）等。
3. **局限性**：
   - ANI缺乏理解、意识和自主决策的能力，只是按照预设的算法和数据进行操作。

### AGI（通用人工智能）

1. **定义**：
   - AGI是指能够理解、学习、适应以及在各种任务中以人类水平或超越人类水平表现的人工智能系统。AGI具有人类智力的广泛特征。
2. **特征**：
   - **灵活性**：AGI能够处理各种不同的任务，包括学习新任务和环境，而不仅仅局限于特定领域。
   - **自主性**：AGI能够进行抽象思维、自主决策和解决问题。
   - **实例**：目前还没有真正实现的 AGI，理论上，AGI可以像人类一样工作、学习和社交。
3. **挑战**：
   - AGI的实现面临许多技术和伦理挑战，包括如何设计具有复杂推理和自我意识的系统。
   - 还有关于安全性、控制问题和潜在的社会影响等方面的讨论。

### 小结

- **ANI** 和 **AGI** 代表了当前人工智能研究中的两种不同层级。ANI专注于特定领域的能力，而AGI则旨在创建可以像人类一样进行通用智能任务的系统。
- ANI 是目前实际应用中存在的类型，而 AGI 仍然是科学家和研究者希望实现的目标。

# 矩阵相关

## 矩阵的乘法

### 运算规则

矩阵A * 矩阵 B = 矩阵 C

用i表示行，j表示列
$$
C_{[i][j]} = A_{[i][:]} \cdot B_{[:][j]}
$$

### **运算的前提**

A矩阵的列号必须等于B矩阵的行号
$$
A_{a,b} \cdot B_{b,c} = C_{a,c}
$$

### **乘法的运算顺序**

- 矩阵乘法是**不可交换**的，即 A*⋅*B*!=*B*⋅*A，除非在特定情况下（如当两个矩阵是对称矩阵时）。

### **乘法的结合律与分配律**

- 矩阵乘法有如下性质：
  - **结合律**：*A*⋅(*B*⋅*C*)=(*A*⋅*B*)⋅*C*
  - **分配律**：*A*⋅(*B*+*C*)=*A*⋅*B*+*A*⋅*C*

## 矩阵的转置

其实就是行列转换用T表示
$$
A^T
$$

### 运算律

$$
C^T = (A \cdot B)^T = B^T \cdot A^T
$$

## 代码实现

```
import torch  

# 定义矩阵 A (3x2)  
A = torch.tensor([[1, 2],   
                  [3, 4],   
                  [5, 6]])  

# 定义矩阵 B (2x3)  
B = torch.tensor([[6, 5, 4],   
                  [3, 2, 1]])  

# 矩阵的乘法  
C = torch.matmul(A, B)  
#print(f'C = A * B\n', C)

#矩阵的转置,只有一行的矩阵被视为行向量)
A_T = A.t()
#print(f'A^T = ',A_T)
```

# **训练神经网络**

**训练神经网络的基本步骤**

- **定义输出**：确定如何通过输入特征和模型参数计算输出。
- 损失函数和成本函数
  - **损失函数**：评估模型在个别训练示例上的表现，通过比较模型输出和真实标签计算错误程度。
  - **成本函数**：所有训练示例损失的平均值，反映模型整体性能。

**使用梯度下降法优化模型**

- 包含几个关键步骤：指定输出计算、定义损失函数及最小化成本函数。
- **模型编译**：选择合适的损失函数（如二元交叉熵）来评估神经网络的性能。

**参数和损失函数的明确**

- 在神经网络构建过程中，必须明确模型的参数和损失函数，以确保 PyTorch 能有效计算输出并优化模型。

**回归问题的损失函数**

- 通过使用**均方误差损失函数**来最小化误差，优化模型参数以提升预测准确性。
- 交叉熵损失函数适用于二元分类任务，强调输出的准确性。

**反向传播算法和优化**

- 神经网络的训练依赖于反向传播算法，PyTorch 提供了高效的实现方式，通过 `torch.autograd` 实现自动微分。
- 利用梯度下降法计算偏导数，用户可高效调整模型参数。

**深度学习库的演变**

- 随着技术的进步，数据科学家和工程师更倾向于使用成熟的深度学习库（如 PyTorch），而不是从头实现。
- 理解深度学习库的内部机制有助于开发者在遇到问题时更有效地调试和修复。

# 激活函数

## 1. 激活函数的重要性

- 激活函数在神经网络中至关重要，直接影响模型的有效性和准确性。适当选择激活函数可以显著提升模型的学习能力和最终预测结果。

## 2. 激活函数的应用

- 输出层
  - **二进制分类问题**：建议使用 Sigmoid 激活函数，有效预测 y=1 的概率。
  - **回归问题**：推荐线性激活函数，以更好地反映连续变量变化（如股票价格）。
  - **非负值预测**（如房价）：使用 ReLU 激活函数，确保输出非负值。
- 隐藏层
  - 默认选择为 ReLU 激活函数，因其高计算效率和快速收敛性。虽然 Sigmoid 在某些情况下有效，但 ReLU 通常表现更优。

ps: ReLU 激活函数有效防止网络隐藏层线性化，提升模型学习能力及特征表达能力，允许处理更复杂的特征和分类问题。

## 3. 学习速度与效率

- 使用 ReLU 激活函数有助于加快学习速度，特别是在使用梯度下降法的训练过程中。理解学习速度的变化有助于进一步优化训练效率。

## 4. 激活函数的多样化

- 尝试多种激活函数（如 Tanh 和泄漏 ReLU）可以提高模型的表现，帮助神经网络处理复杂模式和数据。激活函数的关键在于引入非线性特征。

## 5. 激活函数的局限性

- 激活函数是神经网络的核心；若所有神经元使用线性激活函数，整个网络仅表现为线性回归模型，无法处理复杂数据。
- 线性激活函数限制了网络的表达能力，输入和输出仅呈线性关系，导致模型无法拟合复杂数据分布。

## 6. 多分类问题的扩展

- 神经网络不仅可用于二进制分类，还可拓展到多分类问题。理解如何设计网络以处理多分类任务是构建复杂模型的关键。



# 多类

多类分类问题是指输出标签不止两个的分类问题，例如识别手写数字或分类患者的疾病。

与二元分类不同，多类分类允许y有多个离散类别。通过使用**softmax**回归算法，可以估计每个类别的概率，并将数据集划分为多个类别。
       -手写数字识别是多类分类问题的一个例子。在这个场景中，我们需要识别十个不同的数字，这与只区分两个类别的二元分类不同。
       -在医疗领域，多类分类可以用于诊断多种疾病。例如，医生可以根据症状判断患者是否可能患有三种或五种不同的疾病，这也是多类分类问题的体现。
       -工业检测中，视觉缺陷检测是另一个多类分类应用。工厂可以通过图像识别不同类型的缺陷，如划痕、变色或缺口，以确保产品质量。



## softmax

**定义**：Softmax回归（也称为多项逻辑回归）用于处理多分类问题。它将每个类别的线性组合映射到一个概率值，所有类别的概率和为1。

**数学原理**：

- 假设有K个类别，对于输入特征向量**x**，softmax函数定义如下：

$$
P(y = k | x) = \frac {e^{w_k^Tx}} {\sum_{j = 1}^{K} e^{w_k^Tx}}
$$

- 确保了每个类别的输出概率在0到1之间，且所有类别的概率和为1。

**参数选择的重要性**：

- 模型的表现依赖于参数 *w* 和 *b* 的选择，合理的参数设置可以提高分类的准确性。
- Softmax回归模型可以在给定输入特征时评估各个类别的预测概率，从而做出更明智的决策。

**损失函数**：

- Softmax回归使用负对数损失函数来优化模型，使得预测的概率尽可能接近真实标签。
- 损失函数的定义帮助量化模型的错误程度，对于优化算法的选择和训练效果至关重要。

**在神经网络中的应用**：

- Softmax回归为神经网络中的多类分类问题提供了基础。
- 融合Softmax回归到神经网络中，可以提升模型的复杂性与泛化能力，从而实现更高的分类精度。

# 多标签类

在多标签分类问题中，每个样本可以同时属于多个类别。任务是为每个样本生成一个类别标签的集合。例如，对于一个新闻文章，可能同时标记为“政治”、“经济”和“国际”，即一个样本可以有多个标签。

### 输出层设计

- **多类分类**：输出层通常使用SoftMax激活函数，以输出每个类别的概率分布，并且概率总和为1。
- **多标签分类**：输出层通常使用Sigmoid激活函数，以便独立地计算每个类别的概率。这使得每个类别之间相互独立，概率可以超过1。

### 损失函数

- **多类分类**：常用的损失函数是交叉熵损失（Categorical Crossentropy），用于评估模型对单一正确类别的预测能力。
- **多标签分类**：损失函数通常使用二进制交叉熵（Binary Crossentropy），因为每个标签是一个独立的二分类问题。

### 应用场景

- **多类分类**：适用于只需从多个排他类别中选择一种情况，如手写数字识别、物体识别、情感分析等。
- **多标签分类**：适用于需要同时考虑多个类别的情况，如文本分类（一篇文章可能属于多个主题）、图像标记（同一图像可能有多种物体）等。

# Adam算法

Adam算法（Adaptive Moment Estimation）是一种广泛使用的优化算法，主要用于训练深度学习模型。能够自适应地调整每个参数的学习率，是一种在实际应用中表现良好的算法

### 与传统梯度下降的区别

- **学习率调整**：
  - **梯度下降**：使用固定的学习率，可能导致学习不稳定或收敛慢。
  - **Adam**：自适应调整每个参数的学习率，采用学习率逐步减少的方法，更能适应各参数之间的更新速度。
- **动量**：
  - **标准梯度下降**：不使用动量，可能导致在鞍点附近震荡。
  - **Adam**：结合了动量（通过一阶矩的估计）和自适应学习率（通过二阶矩的估计），使得收敛更快。
- **收敛性**：
  - **标准梯度下降**：收敛可能较慢，容易陷入局部最优。
  - **Adam**：通过自适应调整学习率和使用动量，有助于更快和更稳健地收敛。

###  Adam算法的优势

- **效率**：Adam在大规模数据集上效率高，训练速度快。
- **收敛性**：在许多情况下，Adam能以较少的训练轮次获得较好的结果。
- **噪声鲁棒性**：Adam对梯度噪声较为鲁棒，适用于稀疏梯度问题，如自然语言处理和计算机视觉中的模型。
- **常用性**：在许多深度学习任务中，如图像识别、自然语言处理等，Adam算法已成为默认选择。

# 卷积层

卷积层是卷积神经网络（Convolutional Neural Networks, CNNs）中的核心组件，广泛应用于计算机视觉、自然语言处理和其他深度学习任务。卷积层通过卷积操作提取输入数据的局部特征，并在特征图中保留重要信息。

### 1. 卷积层的基本概念

- **卷积操作**：卷积是一种数学运算，通过一个滑动窗口（通常称为卷积核或滤波器）在输入数据（如图像）上进行操作。卷积核在输入数据上“滑动”，每次计算窗口内的加权和，生成特征图（feature map）。
- **输入和输出**：
  - 输入通常是一个多维张量（例如，图像的高度、宽度和通道数）。
  - 输出是一个新的多维张量，也称为特征图，表现出输入中提取的特征信息。

### 2. 卷积层的组成部分

- **卷积核**：卷积核是一个小尺寸的权重矩阵，通常为二维（在处理图像时）。它在输入数据上滑动，通过加权和计算特征图。卷积核的尺寸（如3x3或5x5）和数量（即输出通道数）是卷积层的超参数。
- **步幅（Stride）**：步幅指的是卷积核滑动的步长。设置更大的步幅会使得输出特征图尺寸减小。
- **填充（Padding）**：填充是在输入数据的边缘添加额外的像素。填充可以是“valid”（无填充）或“same”（输出尺寸与输入相同）。使用填充可以防止特征图尺寸过小，保留更多的边缘信息。
- **激活函数**：卷积层通常会在计算卷积后，通过激活函数（如ReLU、Sigmoid等）对结果进行非线性变换，以增强网络的表达能力。

### 3. 卷积层的功能

- **局部连接**：卷积层通过局部连接，专注于输入中的局部区域，有效提取局部特征。这种局部连接的特点使得卷积层比传统的全连接层参数更少，计算更高效。
- **参数共享**：同一个卷积核在整个输入数据上重复使用，减少了模型的复杂性和参数数量。这使得卷积网络在训练时更加高效并减少过拟合的风险。
- **层次特征提取**：通过堆叠多个卷积层，可以学习到越来越复杂的特征。初级卷积层可能捕捉边缘和纹理，中级卷积层可能识别简单形状，高级卷积层则能捕捉更复杂的对象。

### 4. 卷积层的优势

- **空间不变性**：通过对局部区域的卷积操作，卷积层对物体位置的变化有一定的鲁棒性，能够检测到对象的形状和特征，无论它们在图像中的位置如何。
- **参数高效性**：相比于全连接层，卷积层参数更少，更易于训练，这对于大规模数据集尤为重要。
- **特征自适应**：卷积神经网络能够自动学习卷积核和特征，而不需要手动设计特征提取器。

### 5. 卷积层的应用

卷积层广泛应用于许多领域，尤其是：

- **计算机视觉**：如图像分类、物体检测、图像分割等任务。
- **自然语言处理**：在文本分类等任务中，使用一维卷积层提取文本特征。
- **音频处理**：将音频信号转换为频谱图像，进行分类和识别。

# 模型评估

### 1. 模型性能评估的重要性

- 评估模型的性能可帮助了解其有效性，并为后续的优化提供方向。
- 建立系统化的评估方法是确保模型能够适应新数据的关键。

### 2. 数据集的划分

- **训练集和测试集**：一般将数据集分为70%用于训练，30%用于测试。
- **训练集**：用于优化和学习模型参数，确保模型捕捉数据的主要特征和模式。
- **测试集**：用于检验模型在未知数据上的性能，评估其泛化能力。

### 3. 评估指标

- **训练误差**：在训练集上的表现，低训练误差并不一定意味着模型在新数据上的表现同样良好。
- **测试误差**：在测试集上计算的误差，反映模型在未见数据上的表现；高测试误差可能是过拟合的指征。
- **平方误差和逻辑回归损失函数**：可用于计算训练误差和测试误差，帮助判断模型效果。

### 4. 过拟合与泛化能力

- 过拟合现象表现为训练误差低但测试误差高，需要进行优化和正则化以提高模型的泛化能力。
- 通过监控训练误差和测试误差，可以识别潜在的过拟合问题并进行调整。

### 5. 成本函数和参数优化

- 成本函数的最小化是模型训练中的关键步骤，通过优化平方误差和正则化项来提高模型性能。

### 6. 错误分类率和逻辑损失函数

- 在分类任务中，通过比较训练集和测试集的错误分类率，可以更好地评估模型的表现。
- 使用逻辑损失函数计算平均损失，帮助了解模型在未见数据上的效果，确保模型的泛化能力。

### 7. 模型选择

- 选择合适的模型对于解决特定的机器学习问题非常重要，可以自动选择适用的线性模型或多项式模型，以提高预测精度。