---
title: 电影推荐算法
categories: 阅读
date: 2024-10-13 10:20:45
tags: 人工智能

---

# 电影推荐算法

## 1、下载MovieLens数据集

https://files.grouplens.org/datasets/movielens/

### 模型构建大纲

#### 1. 导入必要库和模块

- 导入数据处理和分析的库（如 NumPy、Pandas）。
- 导入深度学习框架（如 TensorFlow、Keras 或 PyTorch）。

#### 2. 数据准备

- 加载数据
  - 从存储中加载预处理好的数据集（用户特征、电影特征和评分）。
- 特征选择
  - 确定用于训练的特征（用户特征和电影特征）。
- 划分数据集
  - 将数据集划分为训练集、验证集和测试集（如 80% 训练，10% 验证，10% 测试）。

#### 3. 定义嵌入层和输入层

- 嵌入层
  - 为用户和电影ID创建嵌入层，指定嵌入维度。
- 输入层
  - 为其他特征（如性别、年龄、职业、电影类型等）创建输入层。

#### 4. 特征处理

- 通过嵌入层对用户和电影的离散特征进行嵌入。
- 将文本特征通过卷积网络或其他方式进行特征提取（如 LSTM、GRU）。
- 对固定特征（如年龄、性别）进行全连接层处理。

#### 5. 特征融合

- 将处理后的用户特征和电影特征进行合并（例如，使用连接或求和）。
- 经过全连接层进一步处理，这可以包含多个隐藏层，以及激活函数（如 ReLU）。

#### 6. 输出层

- 定义最终的输出层，用于预测评分（如回归问题，输出层的激活函数可以设为线性）。

#### 7. 编译模型

- 选择损失函数（如均方误差 MSE）和优化器（如 Adam）。
- 编译模型，准备进行训练。

#### 8. 训练模型

- 使用训练集进行训练，并在验证集上进行验证。
- 设置合适的批次大小和训练轮数，观察训练过程中的损失值和评估指标。

#### 9. 模型评估

- 使用测试集评估模型的性能，计算评估指标（如均方误差、决定系数等）。
- 可视化训练过程和结果，评估模型的拟合效果。

#### 10. 结果解释

- 分析模型输出，解释模型结果和重要特征，为决策提供支持。

<!--more-->

## 2、数据预处理

data_prepare.py

```
import pandas as pd
import re
import pickle

def load_data():
    """
    Load Dataset from File
    """
    #读取User数据
    users_title = ['UserID', 'Gender', 'Age', 'JobID', 'Zip-code']
    users = pd.read_csv('./ml-1m/users.dat', sep='::', header=None, names=users_title, engine = 'python')
    users = users.filter(regex='UserID|Gender|Age|JobID')
    users_orig = users.values
    #改变User数据中性别和年龄
    gender_map = {'F':0, 'M':1}
    users['Gender'] = users['Gender'].map(gender_map)

    age_map = {val:ii for ii,val in enumerate(set(users['Age']))}
    users['Age'] = users['Age'].map(age_map)

    #读取Movie数据集
    movies_title = ['MovieID', 'Title', 'Genres']
    movies = pd.read_csv('./ml-1m/movies.dat', sep='::', header=None, names=movies_title, engine = 'python')
    movies_orig = movies.values
    #将Title中的年份去掉
    pattern = re.compile(r'^(.*)\((\d+)\)$')

    title_map = {val:pattern.match(val).group(1) for ii,val in enumerate(set(movies['Title']))}
    movies['Title'] = movies['Title'].map(title_map)

    #电影类型转数字字典
    genres_set = set()
    for val in movies['Genres'].str.split('|'):
        genres_set.update(val)

    genres_set.add('<PAD>')
    genres2int = {val:ii for ii, val in enumerate(genres_set)}

    #将电影类型转成等长数字列表，长度是18
    genres_map = {val:[genres2int[row] for row in val.split('|')] for ii,val in enumerate(set(movies['Genres']))}

    for key in genres_map:
        for cnt in range(max(genres2int.values()) - len(genres_map[key])):
            genres_map[key].insert(len(genres_map[key]) + cnt,genres2int['<PAD>'])
    
    movies['Genres'] = movies['Genres'].map(genres_map)

    #电影Title转数字字典
    title_set = set()
    for val in movies['Title'].str.split():
        title_set.update(val)
    
    title_set.add('<PAD>')
    title2int = {val:ii for ii, val in enumerate(title_set)}

    #将电影Title转成等长数字列表，长度是15
    title_count = 15
    title_map = {val:[title2int[row] for row in val.split()] for ii,val in enumerate(set(movies['Title']))}
    
    for key in title_map:
        for cnt in range(title_count - len(title_map[key])):
            title_map[key].insert(len(title_map[key]) + cnt,title2int['<PAD>'])
    
    movies['Title'] = movies['Title'].map(title_map)

    #读取评分数据集
    ratings_title = ['UserID','MovieID', 'ratings', 'timestamps']
    ratings = pd.read_csv('./ml-1m/ratings.dat', sep='::', header=None, names=ratings_title, engine = 'python')
    ratings = ratings.filter(regex='UserID|MovieID|ratings')

    #合并三个表
    data = pd.merge(pd.merge(ratings, users), movies)
    
    columns_order = ['UserID', 'Gender', 'Age', 'JobID', 'MovieID','Title', 'Genres' ,'ratings']  # 这里假设是原始列顺序  
    data = data[columns_order]  # 使用指定的列顺序进行重排 

    #将数据分成X和y两张表
    target_fields = ['ratings']
    features_pd, targets_pd = data.drop(target_fields, axis=1), data[target_fields]
     
    features = features_pd.values
    targets_values = targets_pd.values
    
    return title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig

title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig = load_data()

pickle.dump((title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig), open('preprocess.p', 'wb'))
```

## 3、辅助文件

load.py

```
import pickle
import torch
import pandas as pd
import numpy as np

# Save parameters to file  
def save_params(params):  
    pickle.dump(params, open('preprocess.p', 'wb'))  

# Load parameters from file  
def load_params():  
    return pickle.load(open('preprocess.p', mode='rb'))  


# Load parameters  
title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig = load_params()  

# 参数设置  
embed_dim = 32  # 嵌入矩阵的维度  
uid_max = max(features.take(0, 1)) + 1  # 用户ID个数  
gender_max = max(features.take(1, 1)) + 1  # 性别个数  
age_max = max(features.take(2, 1)) + 1  # 年龄类别个数  
job_max = max(features.take(3, 1)) + 1  # 职业个数  
movie_id_max = max(features.take(4, 1)) + 1  # 电影ID个数  
movie_categories_max = max(genres2int.values()) + 1  # 电影类型个数  
movie_title_max = len(title_set)  # 电影名单词个数  

# 组合方法  
combiner = "sum"  
sentences_size = title_count  # = 15  
window_sizes = [2, 3, 4, 5]  # 使用列表替代集合  
filter_num = 8  
movieid2idx = {val[0]: i for i, val in enumerate(movies.values)}  

# 超参  
num_epochs = 5  
batch_size = 256  
dropout_keep = 0.5  
learning_rate = 0.0001  
show_every_n_batches = 20  
save_dir = './save'  

"""  
- title_count: Title字段的长度(15)  
- title_set: Title文本的集合  
- genres2int: 电影类型转数字的字典  
- features: 是输入X  
- targets_values: 是学习目标y  
- ratings: 评分数据集的Pandas对象  
- users: 用户数据集的Pandas对象  
- movies: 电影数据的Pandas对象  
- data: 三个数据集组合在一起的Pandas对象  
- movies_orig: 没有做数据处理的原始电影数据  
- users_orig: 没有做数据处理的原始用户数据  
"""  

def get_tensor(data):  
    df = pd.DataFrame(data)  

    # 处理 DataFrame，将列表填充到相同的长度  
    max_length = max(df[5].apply(len).max(), df[6].apply(len).max())  

    # 创建填充后的列表  
    title_tensor = df[5].apply(lambda x: x + [0] * (max_length - len(x)))  
    genres_tensor = df[6].apply(lambda x: x + [0] * (max_length - len(x)))  

    # 这里不需要用浮点数，直接使用long  
    title_tensor = torch.tensor(title_tensor.tolist(), dtype=torch.long)  
    genres_tensor = torch.tensor(genres_tensor.tolist(), dtype=torch.long)  

    # 提取用户特征  
    user_ids = df.iloc[:, 0].values.astype(np.int64)  # 转换为整型  
    user_genders = df.iloc[:, 1].values.astype(np.int64)  # 转换为整型  
    user_ages = df.iloc[:, 2].values.astype(np.int64)  # 转换为整型  
    user_jobs = df.iloc[:, 3].values.astype(np.int64)  # 转换为整型  

    # 转换标量特征为张量，并确保都为 Long 类型  
    uid_tensor = torch.tensor(user_ids, dtype=torch.long)  
    gender_tensor = torch.tensor(user_genders, dtype=torch.long)  
    age_tensor = torch.tensor(user_ages, dtype=torch.long)  
    job_tensor = torch.tensor(user_jobs, dtype=torch.long)

    movies_features = df[4].values  
    movies_tensor = torch.tensor(movies_features.astype(np.float32), dtype=torch.long)  

    return uid_tensor,gender_tensor,age_tensor, job_tensor,movies_tensor, title_tensor, genres_tensor  

def get_users(data):
    df = pd.DataFrame(data)  
    # 提取用户特征  
    user_ids = df.iloc[:, 0].values.astype(np.int64)  # 转换为整型  
    user_genders = df.iloc[:, 1].values.astype(np.int64)  # 转换为整型  
    user_ages = df.iloc[:, 2].values.astype(np.int64)  # 转换为整型  
    user_jobs = df.iloc[:, 3].values.astype(np.int64)  # 转换为整型  

    # 转换标量特征为张量，并确保都为 Long 类型  
    uid_tensor = torch.tensor(user_ids, dtype=torch.long)  
    gender_tensor = torch.tensor(user_genders, dtype=torch.long)  
    age_tensor = torch.tensor(user_ages, dtype=torch.long)  
    job_tensor = torch.tensor(user_jobs, dtype=torch.long)

    return uid_tensor, gender_tensor, age_tensor, job_tensor

def get_movies(data):  
    df = pd.DataFrame(data)

    # 处理 DataFrame，将列表填充到相同的长度  
    max_length = max(df['Title'].apply(len).max(), df['Genres'].apply(len).max())  

    # 创建填充后的列表  
    title_tensor = df['Title'].apply(lambda x: x + [0] * (max_length - len(x)))  
    genres_tensor = df['Genres'].apply(lambda x: x + [0] * (max_length - len(x)))  

    # 这里不需要用浮点数，直接使用long  
    title_tensor = torch.tensor(title_tensor.tolist(), dtype=torch.long)  
    genres_tensor = torch.tensor(genres_tensor.tolist(), dtype=torch.long) 

    movies_features = df['MovieID'].values  
    movies_tensor = torch.tensor(movies_features.astype(np.float32), dtype=torch.long)

    return movies_tensor, title_tensor, genres_tensor


```

## 4、用户模型

UserFeatureLayer.py

```
import torch  
import torch.nn as nn  
import load as ld  
import pandas as pd  
import numpy as np  
import os

# 载入参数  
embed_dim_uid = 32  
embed_dim_other = 16  
uid_max = ld.uid_max  
age_max = ld.age_max  
gender_max = ld.gender_max  
job_max = ld.job_max  
X = ld.features  

class UserFeatureLayer(nn.Module):  
    def __init__(self):  
        super(UserFeatureLayer, self).__init__()  
        
        # 嵌入层  
        self.uid_embedding = nn.Embedding(uid_max, embed_dim_uid)  
        self.gender_embedding = nn.Embedding(gender_max, embed_dim_other)  
        self.age_embedding = nn.Embedding(age_max, embed_dim_other)  
        self.job_embedding = nn.Embedding(job_max, embed_dim_other)  

        # 计算拼接后的特征维度  
        total_feature_dim = embed_dim_uid + 3 * embed_dim_other  # 32 + 16 + 16 + 16 = 80  
        
        # 全连接层  
        self.fc1 = nn.Linear(total_feature_dim, 128)  
        self.fc2 = nn.Linear(128, 200)  
        self.relu = nn.ReLU()  

    def forward(self, uid, user_gender, user_age, user_job):  
        # 获取嵌入  
        uid_embed = self.uid_embedding(uid)  
        gender_embed = self.gender_embedding(user_gender)  
        age_embed = self.age_embedding(user_age)  
        job_embed = self.job_embedding(user_job)  

        # 拼接特征  
        user_feature_layer = torch.cat((uid_embed, gender_embed, age_embed, job_embed), dim=1)  

        # 通过全连接层获取最终用户特征  
        output = self.fc1(user_feature_layer)  
        output = self.relu(output)  
        output = self.fc2(output)  
        output = self.relu(output)  
        
        return output  


```

## 5、电影特征模型

MovieFeatureLayer.py

```
import torch  
import torch.nn as nn  
import torch.nn.functional as F  
import load as ld  
import pandas as pd  
import numpy as np  
import os  

# 基础参数  
embed_dim = ld.embed_dim  
movie_id_max = ld.movie_id_max  # 电影ID个数  
movie_categories_max = ld.movie_categories_max  # 电影类型个数  
movie_title_max = ld.movie_title_max  # 电影名单词个数  

# 组合方法  
combiner = "sum"  
sentences_size = ld.title_count  # = 15  
window_sizes = [2, 3, 4, 5]  # 使用列表替代集合  
filter_num = 8  
movieid2idx = {val[0]: i for i, val in enumerate(ld.movies.values)}  

# 超参数  
num_epochs = 5  
batch_size = 256  
dropout_keep = 0.5  
learning_rate = 0.0001  
show_every_n_batches = 20  

class MovieFeatureLayer(nn.Module):  
    def __init__(self):  
        super(MovieFeatureLayer, self).__init__()  
        self.movie_id_embedding = nn.Embedding(movie_id_max, embed_dim)  
        self.category_embedding = nn.Embedding(movie_categories_max, embed_dim)  # 新增的类别嵌入层  

        self.fc1 = nn.Linear(embed_dim * 2, 64)  # 电影ID嵌入和类别嵌入拼接后是 2 * embed_dim   
        self.fc2 = nn.Linear(64 + filter_num * len(window_sizes), 200)  # + filter_num * len(window_sizes) 从 CNN 输出  
        self.relu = nn.ReLU()  # 正确的激活函数定义  

    # 合并电影类型的多个嵌入向量  
    def get_movie_categories_layers(self, movie_categories):  
        category_embeddings = self.category_embedding(movie_categories)  
        return torch.mean(category_embeddings, dim=1)  

    # Movie Title 的文本卷积网络实现  
    def get_movie_cnn_layer(self, movie_titles):  
        movie_title_embed_layer = nn.Embedding(movie_title_max, embed_dim)(movie_titles)  
        movie_title_embed_layer = movie_title_embed_layer.unsqueeze(1)  
        pool_layer_lst = []  
        for window_size in window_sizes:  
            conv_layer = F.relu(nn.Conv2d(in_channels=1, out_channels=filter_num, kernel_size=(window_size, embed_dim))(movie_title_embed_layer))  
            maxpool_layer = F.max_pool2d(conv_layer, kernel_size=(conv_layer.size(2), 1), stride=(1, 1))  
            pool_layer_lst.append(maxpool_layer)  
        pool_layer = torch.cat(pool_layer_lst, dim=1)    
        pool_layer_flat = pool_layer.view(pool_layer.size(0), -1)  
        dropout_layer = F.dropout(pool_layer_flat, p=1-dropout_keep, training=True)  
        return pool_layer_flat, dropout_layer  

    # movie 全连接后 返回一个 movie_features  
    def forward(self, movie_ids, movie_categories, movie_titles):  
        movie_id_embed = self.movie_id_embedding(movie_ids)   
        movie_category_embed = self.get_movie_categories_layers(movie_categories)  

        # 拼接电影ID嵌入和电影类别嵌入  
        tmp_features = torch.cat((movie_id_embed, movie_category_embed), dim=1)  
        
        output = self.fc1(tmp_features)  # 正确使用全连接层  
        output = self.relu(output)  # 使用激活函数  

        movie_title_flat, movie_title_dropout = self.get_movie_cnn_layer(movie_titles)   

        movie_features = torch.cat((output, movie_title_dropout), dim=1)  # 拼接  
        output = self.fc2(movie_features)  # 第二次全连接  
        output = self.relu(output)  # 使用激活函数  

        return output   

```

## 6、主模型

MyNet.py

```
import os  
import time  
import torch  
import torch.nn as nn  
import torch.optim as optim  
import numpy as np  
from sklearn.model_selection import train_test_split  
import load as ld  
import MovieFeatureLayer as Model_Movie  
import UserFeatureLayer as Model_User  

def get_batches(Xs, ys, batch_size):  
    for start in range(0, len(Xs), batch_size):  
        end = min(start + batch_size, len(Xs))  
        yield Xs[start:end], ys[start:end]  

class MyModel(nn.Module):  
    def __init__(self, batch_size=256, learning_rate=0.001):  
        super(MyModel, self).__init__()  
        self.batch_size = batch_size  
        self.best_loss = float('inf')  
        self.losses = {'train': [], 'test': []}  

        # 实例化用户和电影特征层模型  
        self.U_model = Model_User.UserFeatureLayer()  
        self.M_model = Model_Movie.MovieFeatureLayer()  

        # 优化器  
        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)  
        self.criterion = nn.MSELoss()  

        # 创建检查点目录  
        self.model_dir = 'model_dir'  
        self.checkpoint_dir = os.path.join(self.model_dir, 'checkpoints')  
        os.makedirs(self.checkpoint_dir, exist_ok=True)  

    def forward(self, uid_tensor, gender_tensor, age_tensor, job_tensor, movies_tensor, genres_tensor, title_tensor):  
        user_features = self.U_model(uid_tensor, gender_tensor, age_tensor, job_tensor)  
        movie_features = self.M_model(movies_tensor, genres_tensor, title_tensor)  
        inference = torch.sum(user_features * movie_features, dim=1)  
        return inference.unsqueeze(1)  

    def save_checkpoint(self, filename='checkpoint.pth'):  
        torch.save({  
            'model_state_dict': self.state_dict(),  
            'optimizer_state_dict': self.optimizer.state_dict(),  
            'best_loss': self.best_loss,  
        }, os.path.join(self.checkpoint_dir, filename))  

    def load_checkpoint(self, filename='checkpoint.pth'):  
        checkpoint = torch.load(os.path.join(self.checkpoint_dir, filename))  
        self.load_state_dict(checkpoint['model_state_dict'])  
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])  
        self.best_loss = checkpoint['best_loss']  
        print("恢复检查点成功")  

    def compute_loss(self, labels, logits):  
        return self.criterion(logits, labels)  

    def compute_metrics(self, labels, logits):  
        return torch.mean(torch.abs(logits - labels))  

    def extract_movie_features(self, data_batches):  
        movie_matrics = []  
        self.M_model.eval()  
        
        with torch.no_grad():  
            for batch in data_batches:  
                x, _ = batch  
                _, _, _, _, movies_tensor, title_tensor, genres_tensor = ld.get_tensor(x)  

                features = self.M_model(movies_tensor, genres_tensor, title_tensor)  
                movie_matrics.append(features.numpy())  
        
        # 处理空结果  
        if len(movie_matrics) == 0:  
            print("Warning: No movie features extracted.")  
            return np.array([])  # 或者使用其他默认值  

        return np.vstack(movie_matrics)  

    def extract_user_features(self, data_batches):  
        user_matrics = []  
        self.U_model.eval()  

        with torch.no_grad():  
            for batch in data_batches:  
                x, _ = batch  
                uid_tensor, gender_tensor, age_tensor, job_tensor, _, _, _ = ld.get_tensor(x)  
                features = self.U_model(uid_tensor, gender_tensor, age_tensor, job_tensor)  
                user_matrics.append(features.numpy())  
        
        # 处理空结果  
        if len(user_matrics) == 0:  
            print("Warning: No user features extracted.")  
            return np.array([])  # 或者使用其他默认值  
            
        return np.vstack(user_matrics)
    
    def train_step(self, x, y):  
        self.optimizer.zero_grad()  
        logits = self.forward(*x)  
        loss = self.compute_loss(y, logits)  
        loss.backward()  
        self.optimizer.step()  
        return loss, logits  

    def train_model(self, features, targets_values, epochs=5, log_freq=50):  
        
        for epoch_i in range(epochs):  
            train_X, test_X, train_y, test_y = train_test_split(features, targets_values, test_size=0.2, random_state=0)  
            train_batches = get_batches(train_X, train_y, self.batch_size)  
            batch_num = len(train_X) // self.batch_size  

            train_start = time.time()  
            avg_loss = 0.0  

            for batch_i in range(batch_num):  
                x, y = next(train_batches)  
                uid_tensor, gender_tensor, age_tensor, job_tensor, movies_tensor, title_tensor, genres_tensor = ld.get_tensor(x)  
                loss, logits = self.train_step((uid_tensor, gender_tensor, age_tensor, job_tensor, movies_tensor, genres_tensor, title_tensor),  
                                               torch.tensor(np.reshape(y, [self.batch_size, 1]), dtype=torch.float32))  

                avg_loss += loss.item()  

                step = self.optimizer.state[self.optimizer.param_groups[0]['params'][0]]['step']  
                if step % log_freq == 0:  
                    rate = log_freq / (time.time() - train_start)  
                    print('Epoch {:>3} Batch {:>4}/{}   Loss: {:.6f}'.format(epoch_i, batch_i, batch_num, loss.item(), rate))  
                    train_start = time.time()  

            self.losses['train'].append(avg_loss / batch_num)  
            self.testing((test_X, test_y), epoch_i)  
            
            train_batches = list(train_batches)  # 将生成器转换为列表  

            mov_batches = train_batches  
            user_batches = train_batches  

            # 提取并保存 movie_matrics  
            movie_matrics = self.extract_movie_features(mov_batches)  
            with open('movie_matrics.npy', 'wb') as f:  
                np.save(f, movie_matrics)  

            # 提取并保存 user_matrics  
            user_matrics = self.extract_user_features(user_batches)  
            with open('user_matrics.npy', 'wb') as f:  
                np.save(f, user_matrics)

    def testing(self, test_dataset, step_num):  
        print("start testing")  
        test_X, test_y = test_dataset  
        test_batches = get_batches(test_X, test_y, self.batch_size)  
        avg_loss = 0.0  

        for batch_i in range(len(test_X) // self.batch_size):  
            x, y = next(test_batches)  
            uid_tensor, gender_tensor, age_tensor, job_tensor, movies_tensor, title_tensor, genres_tensor = ld.get_tensor(x)  
            logits = self.forward(uid_tensor, gender_tensor, age_tensor, job_tensor, movies_tensor, genres_tensor, title_tensor)  
            test_loss = self.compute_loss(torch.tensor(np.reshape(y, [self.batch_size, 1]), dtype=torch.float32), logits)  
            avg_loss += test_loss.item()  

        avg_loss /= (len(test_X) // self.batch_size)  
        print('Model test set loss: {:.6f}'.format(avg_loss))  

        if avg_loss < self.best_loss:  
            self.best_loss = avg_loss  
            print("best loss = {}".format(self.best_loss))  
            self.save_checkpoint()
```

## 7、 功能实现

main.py

```
import pickle
import keras
import MyModel as Main_Model
import load as ld
import numpy as np
import torch
import pandas as pd
import MovieFeatureLayer
import random

def init_model():
    X = ld.features
    Y = ld.targets_values
    my_net.train_model(X,Y)

my_net = Main_Model.MyModel()
#init_model()
my_net.load_checkpoint()
# 载入电影特征矩阵  
movie_matrics = np.load('movie_matrics.npy')  
#print("Movie Matrics:", movie_matrics)  

# 载入用户特征矩阵  
users_matrics = np.load('user_matrics.npy')  
#print("User Matrics:", user_matrics)

movies = ld.movies
movieid2idx = ld.movieid2idx
sentences_size = ld.sentences_size
users = ld.users
movies_orig = ld.movies_orig
users_orig = ld.users_orig

# 指定用户和电影进行评分
#这部分就是对网络做正向传播，计算得到预测的评分
def rating_movie(mv_net, user_id_val, movie_id_val):  
    user_data = users.iloc[[user_id_val - 1]]
    movie_data = movies.iloc[[movie_id_val -1]]
    uid_tensor, gender_tensor, age_tensor, job_tensor = ld.get_users(user_data)

    movies_tensor, title_tensor , genres_tensor = ld.get_movies(movie_data)

    # 进行推理  
    inference_val = mv_net(uid_tensor, gender_tensor, age_tensor, job_tensor, movies_tensor, genres_tensor, title_tensor)  
    
    return inference_val.detach().numpy()  # 返回 NumPy 数组  

"""
res = rating_movie(my_net, 234, 1401)
print(res)
"""

def recommend_same_type_movie(movie_id_val, movieid2idx, movies_orig, movie_matrics, top_k=20):  
    # 确保输入是 PyTorch 张量  
    if isinstance(movie_matrics, np.ndarray):  
        movie_matrics = torch.tensor(movie_matrics, dtype=torch.float32)  

    # 归一化电影矩阵  
    norm_movie_matrics = torch.sqrt(torch.sum(movie_matrics**2, dim=1, keepdim=True))  # 计算L2范数  
    normalized_movie_matrics = movie_matrics / norm_movie_matrics  # 归一化  

    # 推荐同类型的电影  
    probs_embeddings = normalized_movie_matrics[movieid2idx[movie_id_val]].reshape(1, -1)  # 确保形状正确  
    probs_similarity = torch.matmul(probs_embeddings, normalized_movie_matrics.t())  # 计算相似度  
    sim = probs_similarity.squeeze().numpy()  # 将结果转换为 NumPy 数组  

    print("您看的电影是：{}".format(movies_orig[movieid2idx[movie_id_val]]))  
    print("猜你还喜欢看：")  
    
    # 使用概率分布推荐电影  
    p = np.squeeze(sim)  
    p[np.argsort(p)[:-top_k]] = 0  # 仅保留前 top_k 个电影的相似度  

    # 确保归一化前不会出现全零情况  
    if np.sum(p) == 0:  
        print("没有找到可推荐的电影或任意相似度为零。")  
        return []  

    p = p / np.sum(p)  # 进行归一化  

    results = set()  
    while len(results) < 5:  # 确保结果长度为5  
        # 使用 len(p) 而不是硬编码的3883  
        c = np.random.choice(len(p), 1, p=p)[0]  
        results.add(c)  

    for val in results:  
        print(movies_orig[val])  

    return results  

def recommend_your_favorite_movie(user_id_val, movieid2idx, movies_orig, users_matrics, movie_matrics, top_k=10):  
    # 确保输入是 PyTorch 张量  
    if isinstance(movie_matrics, np.ndarray):  
        movie_matrics = torch.tensor(movie_matrics, dtype=torch.float32)  

    if isinstance(users_matrics, np.ndarray):  
        users_matrics = torch.tensor(users_matrics, dtype=torch.float32)  

    # 归一化电影矩阵  
    norm_movie_matrics = torch.sqrt(torch.sum(movie_matrics**2, dim=1, keepdim=True))  # 计算L2范数  
    normalized_movie_matrics = movie_matrics / norm_movie_matrics  # 归一化  

    # 推荐您喜欢的电影  
    probs_embeddings = users_matrics[user_id_val - 1].reshape(1, -1)  # 确保形状正确  
    probs_similarity = torch.matmul(probs_embeddings, normalized_movie_matrics.t())  # 计算相似度  
    sim = probs_similarity.squeeze().numpy()  # 将结果转换为 NumPy 数组  
    
    print("猜您喜欢：")  
    p = np.squeeze(sim)  
    
    # 确保只保留前 top_k 的相似度  
    p[np.argsort(p)[:-top_k]] = 0  
    if np.sum(p) == 0:  
        print("没有可推荐的电影。")  
        return []  
    
    p = p / np.sum(p)  # 进行归一化  

    results = set()  
    while len(results) < 5:  # 确保结果长度为 5  
        c = np.random.choice(len(p), 1, p=p)[0]  # 使用 len(movies_orig) 动态获取电影数量  
        results.add(c)  

    for val in results:  
        print(movies_orig[val])  

    return results  

def recommend_other_favorite_movie(movie_id_val, movieid2idx, movies_orig, users_orig, users_matrics, movie_matrics, top_k=20):  
    # 确保输入是 PyTorch 张量  
    if isinstance(movie_matrics, np.ndarray):  
        movie_matrics = torch.tensor(movie_matrics, dtype=torch.float32)  

    if isinstance(users_matrics, np.ndarray):  
        users_matrics = torch.tensor(users_matrics, dtype=torch.float32)  

    # 选择电影特征  
    probs_movie_embeddings = users_matrics[movieid2idx[movie_id_val]].reshape(1, -1)  

    # 计算用户喜欢此电影的相似度  
    probs_user_favorite_similarity = torch.matmul(probs_movie_embeddings, users_matrics.t())  
    favorite_user_id = np.argsort(probs_user_favorite_similarity.numpy())[-top_k:]  

    # 输出观看的电影信息  
    print("您看的电影是：{}".format(movies_orig[movieid2idx[movie_id_val]]))  
    #print("喜欢看这个电影的人是：{}".format(users_orig[favorite_user_id - 1]))  

    # 获取喜欢此电影的用户的特征  
    probs_users_embeddings = users_matrics[favorite_user_id - 1].reshape(-1, 200)  
    
    # 计算这些用户与电影的相似度  
    probs_similarity = torch.matmul(probs_users_embeddings, movie_matrics.t())  
    sim = probs_similarity.numpy()  

    # 找到用户喜欢的电影  
    p = np.argmax(sim, axis=1)  
    print("喜欢看这个电影的人还喜欢看：")  

    if len(set(p)) < 5:  
        results = set(p)  
    else:  
        results = set()  
        while len(results) < 5:  
            c = p[random.randint(0, top_k - 1)]  # 从前 top_k 个用户中随机挑选  
            results.add(c)  

    for val in results:  
        print(movies_orig[val])  

    return results  

def test():
    print("\n")
    recommend_your_favorite_movie(user_id_val=16, movieid2idx=movieid2idx, movies_orig=movies_orig, users_matrics=users_matrics, movie_matrics=movie_matrics)
    print("\n")
    recommend_same_type_movie(movie_id_val=27, movieid2idx=movieid2idx, movies_orig=movies_orig, movie_matrics=movie_matrics)    
    print("\n")
    recommend_other_favorite_movie(movie_id_val=27, movieid2idx=movieid2idx, movies_orig=movies_orig, users_orig=users_orig, users_matrics=users_matrics, movie_matrics=movie_matrics)  

test()
```

