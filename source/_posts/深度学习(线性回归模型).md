---
title: 深度学习(线性回归模型)
categories: 阅读
date: 2024-10-10 10:20:45
tags: 人工智能
---

# 深度学习(线性回归模型)

## 监督学习

**定义：**

​	映射 x -> y的算法（input ->  output）

提供输入（source）和正确的输出（labels） 通过定义的算法 训练，让他变成仅接受输入就能预测或猜测出输出 



**回归和分类是深度学习中的两大主要任务方向。**

1. **回归**：如前所述，回归任务主要用于预测连续的数值。它会输出一个实数，常见的应用包括房价预测、温度预测、销售额预测等。回归分析帮助我们理解输入变量与输出变量之间的关系。
2. **分类**：分类任务用于将输入数据分配到离散的类别中。常见的应用包括图像分类（例如，识别图片中的物体）、文本分类（例如，垃圾邮件检测）、情感分析等。在分类任务中，模型的输出通常是一个类别标签或者每个类别的概率。

这两种任务在深度学习模型的设计、损失函数的选择及评估指标上有显著的区别。回归一般使用均方误差（MSE）作为损失函数，而分类则常用交叉熵损失。对于评估，回归常用均方根误差（RMSE）等指标，而分类则多用准确率、精确率、召回率等。

<!--more-->

## 非监督学习

非监督学习是一种在没有标签（labels）和明确目标输出的情况下进行学习的机器学习方法。其主要目标是识别和理解数据中的潜在模式、相似性或分组关系。以下是非监督学习的一些关键概念和应用领域：

- **聚类**：聚类算法用于将相似的数据点分组。在市场营销中，可以通过聚类分析划分用户群体，以便准确制定营销策略。此外，在自然语言处理领域，可以将具有相似主题的文章进行分组。
- **异常检测**：非监督学习可用于识别数据中的异常行为或异常点。例如，在网络安全领域，算法可以检测到与正常模式不符的网络活动，从而识别潜在的安全威胁。在金融领域，可以发现异常交易活动，帮助预防欺诈。
- **降维**：降维技术用于将高维数据投影到低维空间，以便于可视化和进一步分析。主成分分析（PCA）和t-SNE是常用的降维技术，它们能够帮助研究者简化数据，同时保留大部分重要信息，使得数据更易于理解。

非监督学习在处理未经标注的数据时具有广泛应用，可以发现数据中的新模式，并揭示潜在的结构，为后续的分析和决策提供支持。



# 线性回归模型

**linear regression**

## 模型Model

形如
$$
f_{w,b}（x） = wx + b
$$
 的线性直线



## 模型的参数 （parameners）

w,b 

模型的参数指 可以在训练期间调整以改进模型的变量 ， 别名权重、系数



## 成本函数

别名 ： 代价函数

为了找到更贴近的w和b，我们将构造一个**成本函数**

**平方误差成本函数**
$$
J(w,b) = \frac{1}{2m} \sum_{i=1}^{m} \left( f_{w,b}(x^{(i)}) - y^{(i)} \right)^2
$$


## 目标

线性回归会尝试找到w，b的值，并尽可能让w，b最小即
$$
minmize J(w,b)
$$

## 梯度下降

梯度下降算法是从一个初始参数（如权重 *w* 和偏置 *b*）开始，通过迭代不断调整这些参数来最小化损失函数。然而，不同的初始参数可能导致梯度下降收敛到不同的局部最小值，尤其是在损失函数具有多个极小值的情况下。

### 实现梯度下降

线性回归中公式如下，注意这里的等于号是 **赋值** 而非 相等
$$
w = w - \alpha \frac{d}{dw} J(w, b)
$$

$$
b = b - \alpha \frac{d}{db} J(w, b)
$$

求导后结果
$$
w = \frac{1}{m} \sum_{i=1}^{m} \left( f_{w,b}(x^{(i)}) - y^{(i)} \right)x^{(i)}
$$

$$
b = \frac{1}{m} \sum_{i=1}^{m} \left( f_{w,b}(x^{(i)}) - y^{(i)} \right)
$$

α：学习率（learning rate），通常是一个0~1之间的小正数

- alpha越大，梯度下降越激烈
- 太小走的慢，太大不一定能到最小值，卡在山谷的两边，甚至发散，离目标值越来越远

​	

w,b两个应该**同步更新**



导数向：表示了损失函数 在参数空间中的变化率和方向

- 正值：如果某个参数的导数为正，说明增加该参数会导致损失增加。这意味着我们应该减少这个参数的值，以期降低损失。
- 负值：如果导数为负，说明增加该参数会导致损失减少。在这种情况下，我们应该增加这个参数的值。
- 零值：当导数为零时，表示此处可能是一个最优解（局部最低点或鞍点），因为在这一点上，损失函数不再变化。



## 构建模型

有一些没见过的后续会提及，不用着急

**test.py**

```
import numpy as np  
import os  
import torch  
import torch.nn as nn  
import torch.optim as optim  
from torch.utils.tensorboard import SummaryWriter  

# 读取数据  
filedata = np.loadtxt('test.txt', dtype=float)  
Areas = filedata[:, 0]  # 面积  
Prices = filedata[:, 1]  # 房价  

# 数据归一化  
Areas = (Areas - np.mean(Areas)) / np.std(Areas)  
Prices = (Prices - np.mean(Prices)) / np.std(Prices)  

# 定义线性回归模型  
class LinearModel(nn.Module):  
    def __init__(self):  
        super(LinearModel, self).__init__()  
        self.linear = nn.Linear(1, 1)  # 输入一个特征，输出一个目标值  

    def forward(self, x):  
        return self.linear(x)  

# 创建模型实例  
model = LinearModel()  

# 准备输入数据  
areas_tensor = torch.from_numpy(Areas).float().view(-1, 1)  # 转换为 PyTorch 张量  
prices_tensor = torch.from_numpy(Prices).float().view(-1, 1)  # 转换为 PyTorch 张量  

# 定义损失函数和优化器  
criterion = nn.MSELoss()  # 均方误差损失  
optimizer = optim.SGD(model.parameters(), lr=0.001)  # 随机梯度下降优化器  

# 创建 TensorBoard SummaryWriter  
writer = SummaryWriter('runs/linear_regression_experiment')  

# 记录模型图  
dummy_input = torch.tensor([[0.0]], dtype=torch.float32)  # 模型输入的占位符  
writer.add_graph(model, dummy_input)  

# 记录实验描述  
writer.add_text('Experiment Description', 'This is a simple linear regression model training on normalized areas and prices.')  

# 训练模型  
epochs = 500  
for epoch in range(epochs):  
    model.train()  # 将模型设置为训练模式  

    # 前向传播  
    optimizer.zero_grad()  # 清零梯度  
    outputs = model(areas_tensor)  # 输出结果  
    loss = criterion(outputs, prices_tensor)  # 计算损失  

    # 反向传播和优化  
    loss.backward()  # 反向传播  
    optimizer.step()  # 更新参数  

    # 在 TensorBoard 中记录损失  
    writer.add_scalar('Loss/train', loss.item(), epoch)  

    # 记录模型参数的直方图  
    for name, param in model.named_parameters():  
        writer.add_histogram(name, param.data, epoch)  
        # 记录参数分布  
        writer.add_histogram(f'{name}/distribution', param.data, epoch)  

    # 每100个epoch打印一次损失  
    if epoch % 100 == 0:  
        print(f"Epoch {epoch}, Loss: {loss.item()}")  

# 保存模型为.pt  
torch.save(model.state_dict(), 'linear_regression_model.pt')  

# 转换为 ONNX 格式  
torch.onnx.export(model, dummy_input, "linear_regression_model.onnx", verbose=True)  

print("模型已保存为 'linear_regression_model.pt' 和 'linear_regression_model.onnx'")  

# 关闭 TensorBoard writer  
writer.close()  

# 启动 TensorBoard，指定端口  
os.system('python -m tensorboard.main --logdir="runs/linear_regression_experiment" --port=8888')
```



```
可以在浏览器中输入 http://localhost:8888/查看报表
```



**创建pytroch模型**

```
import numpy as np  
import torch  
import torch.nn as nn  
import torch.optim as optim  

# 读取数据  
filedata = np.loadtxt('test.txt', dtype=float)  
Areas = filedata[:, 0]  # 面积  
Prices = filedata[:, 1]  # 房价  

# 数据归一化  
Areas = (Areas - np.mean(Areas)) / np.std(Areas)  
Prices = (Prices - np.mean(Prices)) / np.std(Prices)  

# 定义线性回归模型  
class LinearModel(nn.Module):  
    def __init__(self):  
        super(LinearModel, self).__init__()  
        self.linear = nn.Linear(1, 1)  # 输入一个特征，输出一个目标值  

    def forward(self, x):  
        return self.linear(x)  

# 创建模型实例  
model = LinearModel()  

# 准备输入数据  
areas_tensor = torch.from_numpy(Areas).float().view(-1, 1)  # 转换为 PyTorch 张量  
prices_tensor = torch.from_numpy(Prices).float().view(-1, 1)  # 转换为 PyTorch 张量  

# 定义损失函数和优化器  
criterion = nn.MSELoss()  # 均方误差损失  
optimizer = optim.SGD(model.parameters(), lr=0.001)  # 随机梯度下降优化器  

# 训练模型  
epochs = 500  
for epoch in range(epochs):  
    model.train()  # 将模型设置为训练模式  

    # 前向传播  
    optimizer.zero_grad()  # 清零梯度  
    outputs = model(areas_tensor)  # 输出结果  
    loss = criterion(outputs, prices_tensor)  # 计算损失  

    # 反向传播和优化  
    loss.backward()  # 反向传播  
    optimizer.step()  # 更新参数  

    # 每100个epoch打印一次损失  
    if epoch % 100 == 0:  
        print(f"Epoch {epoch}, Loss: {loss.item()}")  

# 保存模型为.pt  
torch.save(model.state_dict(), 'linear_regression_model.pt')  

# 转换为 ONNX 格式  
dummy_input = torch.tensor([[0.0]], dtype=torch.float32)  # 模型输入的占位符  
torch.onnx.export(model, dummy_input, "linear_regression_model.onnx", verbose=True)  

print("模型已保存为 'linear_regression_model.pt' 和 'linear_regression_model.onnx'")
```



**测试pt模型正确性**

testpt.py

```
import sys  
import numpy as np  
import torch  
import torch.nn as nn  

# 定义线性回归模型  
class LinearModel(nn.Module):  
    def __init__(self):  
        super(LinearModel, self).__init__()  
        self.linear = nn.Linear(1, 1)  # 输入一个特征，输出一个目标值  

    def forward(self, x):  
        return self.linear(x)  

def load_model(model_path):  
    """  
    加载模型  
    """  
    model = LinearModel()  
    model.load_state_dict(torch.load(model_path))  
    model.eval()  # 设置为评估模式  
    return model  

def predict_price(model, area, filedata):  
    """  
    预测房价  
    """  
    # 归一化输入  
    areas = filedata[:, 0]  
    area_normalized = (area - np.mean(areas)) / np.std(areas)  

    # 将输入转换为 PyTorch 的张量  
    area_tensor = torch.tensor([[area_normalized]], dtype=torch.float32)  

    # 进行预测  
    with torch.no_grad():  
        predicted_price_normalized = model(area_tensor)  

    # 反归一化输出  
    prices = filedata[:, 1]  
    predicted_price = predicted_price_normalized.item() * np.std(prices) + np.mean(prices)  
    
    return predicted_price  

if __name__ == "__main__":  
    if len(sys.argv) != 3:  
        print("用法: python predict.py <model_path> <area>")  
        sys.exit(1)  

    model_path = sys.argv[1]  
    area = float(sys.argv[2])  

    # 读取数据  
    filedata = np.loadtxt('test.txt')  

    # 加载模型  
    model = load_model(model_path)  

    # 进行预测  
    price = predict_price(model, area, filedata)  

    # 打印结果  
    print(f"预测房价(面积 {area} 平方米): {price} 万")
```



**测试onnx模型正确性**

```
#include <iostream>  
#include <vector>  
#include <string>  
#include <opencv2/opencv.hpp>  
#include <opencv2/dnn/dnn.hpp>  

using cv::Mat;  
using std::vector;  
using std::string;  
using std::cout;  
using std::endl;  

// 从您计算的结果中填入均值和标准差  
const float mean_area = 113.7988f; // 房屋面积均值  
const float std_area = 45.4836f;   // 房屋面积标准差  
const float mean_price = 1176.76f; // 房价均值  
const float std_price = 1118.3278f; // 房价标准差 

int main(int argc, char* argv[]) {  
    if (argc != 2) {  
        cout << "Usage: " << argv[0] << " <area>" << endl;  
        return -1;  
    }  

    // 解析输入参数  
    float area = std::stof(argv[1]);  

    // 读取 ONNX 模型  
    cv::dnn::Net net = cv::dnn::readNetFromONNX("linear_regression_model.onnx");   

    // 归一化输入数据  
    float normalized_area = (area - mean_area) / std_area;  

    // 创建输入张量  
    Mat inputBlob = Mat(1, 1, CV_32F, normalized_area);  
    net.setInput(inputBlob);  

    // 执行推理  
    Mat output = net.forward();  

    // 反归一化输出  
    float predicted_price_normalized = output.at<float>(0, 0);  
    float predicted_price = predicted_price_normalized * std_price + mean_price;  

    // 输出结果  
    cout << "预测房价(面积 " << area << " 平方米): " << predicted_price << " 万" << endl;  

    return 0;  
}
```

追加：

**计算均差和标准差**

tmp.py

```
import numpy as np  

# 读取数据  
data = np.loadtxt('test.txt')  

# 计算均值和标准差  
mean_area = np.mean(data[:, 0])  # 房屋面积的均值  
std_area = np.std(data[:, 0])    # 房屋面积的标准差  
mean_price = np.mean(data[:, 1]) # 房价的均值  
std_price = np.std(data[:, 1])   # 房价的标准差  

print("Mean Area:", mean_area)  
print("Std Area:", std_area)  
print("Mean Price:", mean_price)  
print("Std Price:", std_price)
```



# 多输入特征的多元线性回归

之前说的线性回归模型只有单一特征值，现在他有多特征，用**下标**表示即
$$
x_i
$$
ps:分清楚上标和下标的区别，上标指是集合中第i个元素，下标指元素的第i个特征

比如第四个数据的第2个特征，我们用下面形式表示
$$
x_4^{(2)}
$$

## Model

$$
f_{w,b}(x) = w \cdot x +b
$$

ps：这里是**点积**



## 向量化

python调用numpy库

实现向量点积

```
f = np.dot(w,x) + b
```

向量化的速度比直接用for循环要快的多

```
for i in range(0,n)
	f += w[i] * x[i]
f += b
```

why?

1. **减少了Python解释器的开销**：在循环中，每次迭代都需要解释器进行上下文切换和多次函数调用，而在向量化操作中，这些开销大大减少，因为整个操作是在底层C或Fortran实现的，调用效率更高。
2. **使用优化的底层实现**：NumPy等科学计算库通常使用底层的优化算法（例如，BLAS和LAPACK）来处理向量和矩阵运算。这些算法经过高度优化，可以利用现代CPU的特性，例如SIMD（单指令多数据），从而加速运算。
3. **内存访问模式**：向量化操作通常会更好地利用缓存，因为它可以在连续的内存块中处理数据。相比之下，循环可能导致更多的随机内存访问，这会导致缓存不命中，从而降低性能。
4. **最主要一点**，np.dot实现了并行计算，并使用专门的硬件进行相加



## 梯度下降

$$
w_j = w_j - \alpha\frac{\partial}{\partial w_j} J(w_1, \ldots, w_n, b)
$$

$$
w_j = w_j - \alpha\frac{\partial}{\partial b} J(w_1, \ldots, w_n, b)
$$

ps:这个长得像6的是偏导数



## 特征缩放（归一化）

不同特征可能具有不同的取值范围，当特征值的差异很大时，梯度下降等优化算法可能会在某一方向上反应过度，导致反复 oscillation（震荡）而无法快速收敛，这会延长训练时间这会导致在训练模型时。

解决办法 ：

​	 **归一化**

​	通过特征缩放，可以将数据统一到一个相似的尺度，比如将特征值归一化到0到1之间



​	**标准化**（Z-score Normalization）

​	每个数减去平均数，再除以标准差（方差的平方根）
$$
x_i = \frac {(x_i - \mu _i)} {\sigma _i}
$$
​	我们常用以下字符表示标准差（sigma）
$$
\sigma
$$
​	以下字符表示平均值（mu）
$$
\mu
$$
​	python代码

```
x = (x - np.mean(x))/np.std(x)

x是向量数组
```



## 检测梯度下降是否收敛

1、看学习曲线（learning curve）

x轴为训练次数

y轴为J(w,b)

若是下降的曲线则是收敛的



## 特征工程

特征工程是机器学习和数据挖掘过程中一个关键的步骤，旨在通过选择、修改和创建特征来提高模型的性能。特征是模型输入的数据属性，特征工程的质量直接影响模型的效果。

### 1. 特征选择（Feature Selection）

- **目的**：从现有特征中挑选出对模型预测最有帮助的特征，减少冗余和噪声，提高模型的泛化能力。
- 方法
  - 滤波法（Filter method）：使用统计测试和相关性分析来评估特征。
  - 包装法（Wrapper method）：使用某种机器学习算法评估特征子集。
  - 嵌入法（Embedded method）：结合模型学习过程进行特征选择，例如Lasso回归。

### 2. 特征提取（Feature Extraction）

- **目的**：通过原始数据生成新的特征，通常用于减少维度，保持必要的信息。
- 方法
  - 主成分分析（PCA）：将高维数据映射到低维空间。
  - 线性判别分析（LDA）：试图找到最佳投影方向以提高类间分离。

### 3. 特征构造（Feature Engineering）

- **目的**：基于现有特征生成新的特征，以增强模型的表达能力。
- 方法
  - 数学变换：如对数变换、平方、平方根等。
  - 类别编码：将类别变量转换为数值形式（如独热编码、标签编码）。
  - 交互特征：构造特征之间的交互项（如乘积、比率等）。

### 4. 数据清洗（Data Cleaning）

- **目的**：处理缺失值、异常值和噪声，以提高数据质量。
- 方法
  - 处理缺失值：填补缺失值、删除含缺失值的样本等。
  - 异常检测和处理：识别并处理不合逻辑或极端的数据点。

### 5. 特征缩放（Feature Scaling）

- **目的**：将特征调整到相似的尺度，以避免某些特征在模型训练中占主导地位。
- 方法
  - 归一化（Normalization）：将数据调整到 0 到 1 之间。
  - 标准化（Standardization）：将数据调整为均值为0、标准差为1的分布。



# 多项式回归概述

**多项式回归（Polynomial Regression）**是一种扩展传统线性回归的方法，用于建立输入特征与输出目标之间的非线性关系。它通过引入多项式特征，可以有效地拟合复杂的数据模式。



## 1. 特征工程在多项式回归中的作用

特征工程对于多项式回归的成功至关重要，主要包括以下方面：

- **特征构造**：
  - 通过将原始特征 x 转换为其高次项（例如 x^2,x^3等），增强模型的表达能力。
- **特征选择**：
  - 精心选择高次项的级别，以避免过拟合。过多的高次特征可能导致模型的复杂性增高，从而降低模型在新数据上的表现。可以使用交叉验证等方法来评估不同多项式次数的效果。
- **交互特征**：
  - 在某些情况下，可以考虑特征间的交互作用，这有助于捕捉变量之间的关联性。
- **数据理解**：
  - 对数据的特点和背景知识的理解，可以帮助决定哪些特征需要上升到高次，以更好地拟合数据。

## 2. 应用场景

多项式回归常用于以下情境：

- 数据呈现出明显的非线性关系。
- 需要在模型中考虑高阶效应。
- 希望在保持可解释性的同时捕获更多的复杂性。



### 特征的次方

- **非线性关系**：特征的次方（如 x1^2）可以帮助模型捕捉到特征与输出之间的非线性关系。例如，房子的面积平方可能更好地反映价格的变化趋势。
- **曲线拟合**：通过增加次方项，模型可以拟合更复杂的曲线，而不仅仅是直线。

### 特征的相乘（交互项）

- **特征交互**：交互项（如 x*1⋅*x2）用于表示两个特征之间的相互作用。例如，面积和地理位置的交互可能影响房价的合理性。
- **复杂关系**：这些项可以揭示单个特征无法单独捕捉到的复杂关系。



### 实例

预测售价 售价 = 面积 + 地理位置 + 房间数量 

很明显，面积和地理位置是相辅相成的， 但房间数量是不能引发线性关系的 那我就可以设成如下方程
$$
y(x) = wx_1x_2 + x_3^k
$$
其中

​	

- **偶数次方（k=2,4,...**：
  - 表示特征增加时售价影响为正向，且以平滑且加速的方式提升，适合描述稳定的增长。
- **奇数次方（k=1,3,...**：
  - 可能表现出复杂的正向和负向影响，适合用来捕捉某些特征与结果之间的非线性关系，并且可能存在一定的上下波动。

## 结论

多项式回归是一种强大的回归分析方法，通过引入多项式高次项，可以有效处理非线性关系。特征工程在多项式回归中扮演着重要的角色，适当的特征构造和选择能够显著改善模型的表现和准确性。然而，必须谨慎选择多项式的复杂度，以防止过拟合并确保模型的可解释性。

虽然多项式回归在某些特定领域（如物理科学、经济学等）仍然有其应用价值，但在实际的机器学习和数据分析场景中，它**并不是最常用**的模型。在选择模型时，通常会优先考虑更灵活、更易于处理的非线性模型。因此，在处理复杂数据时，考虑其他更先进和更棒的模型通常是一个更为常见的选择。

